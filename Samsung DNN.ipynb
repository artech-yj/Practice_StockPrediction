{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 불러오기\n",
    "df = pd.read_csv('./data/samsung.csv', encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>57,800</td>\n",
       "      <td>58,400</td>\n",
       "      <td>56,400</td>\n",
       "      <td>56,400</td>\n",
       "      <td>19,749,457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>58,800</td>\n",
       "      <td>58,800</td>\n",
       "      <td>56,800</td>\n",
       "      <td>57,200</td>\n",
       "      <td>20,821,939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>59,100</td>\n",
       "      <td>59,700</td>\n",
       "      <td>58,800</td>\n",
       "      <td>59,100</td>\n",
       "      <td>16,446,102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>59,400</td>\n",
       "      <td>59,400</td>\n",
       "      <td>58,300</td>\n",
       "      <td>58,800</td>\n",
       "      <td>23,664,541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>61,800</td>\n",
       "      <td>61,800</td>\n",
       "      <td>60,700</td>\n",
       "      <td>60,800</td>\n",
       "      <td>14,916,555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           일자      시가      고가      저가      종가         거래량\n",
       "0  2020-01-31  57,800  58,400  56,400  56,400  19,749,457\n",
       "1  2020-01-30  58,800  58,800  56,800  57,200  20,821,939\n",
       "2  2020-01-29  59,100  59,700  58,800  59,100  16,446,102\n",
       "3  2020-01-28  59,400  59,400  58,300  58,800  23,664,541\n",
       "4  2020-01-23  61,800  61,800  60,700  60,800  14,916,555"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터의 구성을 확인하기 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>2018-05-11</td>\n",
       "      <td>52,000</td>\n",
       "      <td>52,200</td>\n",
       "      <td>51,200</td>\n",
       "      <td>51,300</td>\n",
       "      <td>10,314,997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>2018-05-10</td>\n",
       "      <td>51,700</td>\n",
       "      <td>51,700</td>\n",
       "      <td>50,600</td>\n",
       "      <td>51,600</td>\n",
       "      <td>13,905,263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>2018-05-09</td>\n",
       "      <td>52,600</td>\n",
       "      <td>52,800</td>\n",
       "      <td>50,900</td>\n",
       "      <td>50,900</td>\n",
       "      <td>16,128,305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>52,600</td>\n",
       "      <td>53,200</td>\n",
       "      <td>51,900</td>\n",
       "      <td>52,600</td>\n",
       "      <td>23,104,720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>53,000</td>\n",
       "      <td>53,900</td>\n",
       "      <td>51,800</td>\n",
       "      <td>51,900</td>\n",
       "      <td>39,565,391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             일자      시가      고가      저가      종가         거래량\n",
       "421  2018-05-11  52,000  52,200  51,200  51,300  10,314,997\n",
       "422  2018-05-10  51,700  51,700  50,600  51,600  13,905,263\n",
       "423  2018-05-09  52,600  52,800  50,900  50,900  16,128,305\n",
       "424  2018-05-08  52,600  53,200  51,900  52,600  23,104,720\n",
       "425  2018-05-04  53,000  53,900  51,800  51,900  39,565,391"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터가 얼마나 있는지 확인하기\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#날짜를 기준으로 데이터 정렬해주기\n",
    "df=df.sort_values(['일자'], ascending = [True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 값들이 전부 문자열이므로 숫자들로 바꿔주기\n",
    "for col in df.columns[1:]:  # Iterate over chosen columns\n",
    "    df[col] = df[col].str.replace(\",\",\"\")\n",
    "    df[col] = pd.to_numeric(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size = 0.2, shuffle  = False) #train : 298행  / test : 128행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(train, test_size = 0.2, shuffle  = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>53000</td>\n",
       "      <td>53900</td>\n",
       "      <td>51800</td>\n",
       "      <td>51900</td>\n",
       "      <td>39565391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>52600</td>\n",
       "      <td>53200</td>\n",
       "      <td>51900</td>\n",
       "      <td>52600</td>\n",
       "      <td>23104720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>2018-05-09</td>\n",
       "      <td>52600</td>\n",
       "      <td>52800</td>\n",
       "      <td>50900</td>\n",
       "      <td>50900</td>\n",
       "      <td>16128305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>2018-05-10</td>\n",
       "      <td>51700</td>\n",
       "      <td>51700</td>\n",
       "      <td>50600</td>\n",
       "      <td>51600</td>\n",
       "      <td>13905263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>2018-05-11</td>\n",
       "      <td>52000</td>\n",
       "      <td>52200</td>\n",
       "      <td>51200</td>\n",
       "      <td>51300</td>\n",
       "      <td>10314997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             일자     시가     고가     저가     종가       거래량\n",
       "425  2018-05-04  53000  53900  51800  51900  39565391\n",
       "424  2018-05-08  52600  53200  51900  52600  23104720\n",
       "423  2018-05-09  52600  52800  50900  50900  16128305\n",
       "422  2018-05-10  51700  51700  50600  51600  13905263\n",
       "421  2018-05-11  52000  52200  51200  51300  10314997"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#일자를 기준으로 순서 정렬\n",
    "train = train.sort_values(['일자'], ascending = [True])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'일자'열은 학습에 필요없으니 지워주기\n",
    "del train['일자']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>53000</td>\n",
       "      <td>53900</td>\n",
       "      <td>51800</td>\n",
       "      <td>51900</td>\n",
       "      <td>39565391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>52600</td>\n",
       "      <td>53200</td>\n",
       "      <td>51900</td>\n",
       "      <td>52600</td>\n",
       "      <td>23104720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>52600</td>\n",
       "      <td>52800</td>\n",
       "      <td>50900</td>\n",
       "      <td>50900</td>\n",
       "      <td>16128305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>51700</td>\n",
       "      <td>51700</td>\n",
       "      <td>50600</td>\n",
       "      <td>51600</td>\n",
       "      <td>13905263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>52000</td>\n",
       "      <td>52200</td>\n",
       "      <td>51200</td>\n",
       "      <td>51300</td>\n",
       "      <td>10314997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>44800</td>\n",
       "      <td>45000</td>\n",
       "      <td>44550</td>\n",
       "      <td>44850</td>\n",
       "      <td>6664872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>44800</td>\n",
       "      <td>45050</td>\n",
       "      <td>44300</td>\n",
       "      <td>44600</td>\n",
       "      <td>8607439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>44200</td>\n",
       "      <td>44400</td>\n",
       "      <td>43400</td>\n",
       "      <td>43750</td>\n",
       "      <td>16906541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>43750</td>\n",
       "      <td>44150</td>\n",
       "      <td>43300</td>\n",
       "      <td>44000</td>\n",
       "      <td>9322873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>43750</td>\n",
       "      <td>44050</td>\n",
       "      <td>43400</td>\n",
       "      <td>43900</td>\n",
       "      <td>11890424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        시가     고가     저가     종가       거래량\n",
       "425  53000  53900  51800  51900  39565391\n",
       "424  52600  53200  51900  52600  23104720\n",
       "423  52600  52800  50900  50900  16128305\n",
       "422  51700  51700  50600  51600  13905263\n",
       "421  52000  52200  51200  51300  10314997\n",
       "..     ...    ...    ...    ...       ...\n",
       "158  44800  45000  44550  44850   6664872\n",
       "157  44800  45050  44300  44600   8607439\n",
       "156  44200  44400  43400  43750  16906541\n",
       "155  43750  44150  43300  44000   9322873\n",
       "154  43750  44050  43400  43900  11890424\n",
       "\n",
       "[272 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'일자'가 사라졌는지 확인하기\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train에서 학습용 y_train 분리\n",
    "y_train = train['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train에서 학습용 y_train 분리한 다음 '종가'열 지워주기\n",
    "del train['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train에서 종가열이 사라졌으니 x_train으로 이름 변경\n",
    "x_train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation 데이터도 마찬가지\n",
    "y_val = validation['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del validation['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_val['일자']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2019-09-25</td>\n",
       "      <td>49200</td>\n",
       "      <td>49350</td>\n",
       "      <td>48800</td>\n",
       "      <td>48900</td>\n",
       "      <td>9187141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2019-09-26</td>\n",
       "      <td>49000</td>\n",
       "      <td>49250</td>\n",
       "      <td>48900</td>\n",
       "      <td>49200</td>\n",
       "      <td>8494756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>48000</td>\n",
       "      <td>48700</td>\n",
       "      <td>48000</td>\n",
       "      <td>48400</td>\n",
       "      <td>8048041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>48050</td>\n",
       "      <td>49250</td>\n",
       "      <td>47900</td>\n",
       "      <td>49050</td>\n",
       "      <td>9497119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>48900</td>\n",
       "      <td>49100</td>\n",
       "      <td>48650</td>\n",
       "      <td>48850</td>\n",
       "      <td>6206035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            일자     시가     고가     저가     종가      거래량\n",
       "85  2019-09-25  49200  49350  48800  48900  9187141\n",
       "84  2019-09-26  49000  49250  48900  49200  8494756\n",
       "83  2019-09-27  48000  48700  48000  48400  8048041\n",
       "82  2019-09-30  48050  49250  47900  49050  9497119\n",
       "81  2019-10-01  48900  49100  48650  48850  6206035"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#일자를 기준으로 순서 정렬\n",
    "test = test.sort_values(['일자'], ascending = [True])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_test['일자']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#Keras 모듈 불러오기\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 45        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 18        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 410\n",
      "Trainable params: 410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#모델 구성하기\n",
    "model = Sequential()#---------------------모델 방식\n",
    "\n",
    "model.add(Dense(10, input_shape = (4,))) #-Hidden Layer 1\n",
    "model.add(Dense(15))#---------------------Hidden Layer 2\n",
    "model.add(Dense(8)) #---------------------Hidden Layer 3\n",
    "model.add(Dense(5)) #---------------------Hidden Layer 4\n",
    "model.add(Dense(3)) #---------------------Hidden Layer 5\n",
    "model.add(Dense(1)) #---------------------Output Layer\n",
    "\n",
    "model.summary() #-------------------------레이어 형태&파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 272 samples, validate on 68 samples\n",
      "Epoch 1/200\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 193351189460.5367 - mse: 193351204864.0000 - val_loss: 858547355.5294 - val_mse: 858547392.0000\n",
      "Epoch 2/200\n",
      "272/272 [==============================] - 0s 718us/step - loss: 803010287.3676 - mse: 803010176.0000 - val_loss: 805793407.4265 - val_mse: 805793344.0000\n",
      "Epoch 3/200\n",
      "272/272 [==============================] - 0s 663us/step - loss: 3081615438.0441 - mse: 3081614848.0000 - val_loss: 290855484.5294 - val_mse: 290855456.0000\n",
      "Epoch 4/200\n",
      "272/272 [==============================] - 0s 717us/step - loss: 1072364492.6278 - mse: 1072364352.0000 - val_loss: 9299203681.8824 - val_mse: 9299205120.0000\n",
      "Epoch 5/200\n",
      "272/272 [==============================] - 0s 734us/step - loss: 3239925734.5147 - mse: 3239925760.0000 - val_loss: 466350241.4412 - val_mse: 466350272.0000\n",
      "Epoch 6/200\n",
      "272/272 [==============================] - 0s 769us/step - loss: 1141738642.5441 - mse: 1141738624.0000 - val_loss: 427599653.9706 - val_mse: 427599616.0000\n",
      "Epoch 7/200\n",
      "272/272 [==============================] - 0s 838us/step - loss: 492644756.2537 - mse: 492644608.0000 - val_loss: 251914660.2941 - val_mse: 251914624.0000\n",
      "Epoch 8/200\n",
      "272/272 [==============================] - 0s 902us/step - loss: 1571950011.2243 - mse: 1571949952.0000 - val_loss: 264126294.5882 - val_mse: 264126320.0000\n",
      "Epoch 9/200\n",
      "272/272 [==============================] - 0s 807us/step - loss: 1518422183.4926 - mse: 1518422144.0000 - val_loss: 463583657.7206 - val_mse: 463583648.0000\n",
      "Epoch 10/200\n",
      "272/272 [==============================] - 0s 849us/step - loss: 2347409840.5919 - mse: 2347409408.0000 - val_loss: 378797467.5882 - val_mse: 378797408.0000\n",
      "Epoch 11/200\n",
      "272/272 [==============================] - 0s 886us/step - loss: 1857361526.0515 - mse: 1857361664.0000 - val_loss: 448546150.3529 - val_mse: 448546144.0000\n",
      "Epoch 12/200\n",
      "272/272 [==============================] - 0s 862us/step - loss: 4289055289.5358 - mse: 4289056000.0000 - val_loss: 356132991.9412 - val_mse: 356132992.0000\n",
      "Epoch 13/200\n",
      "272/272 [==============================] - 0s 732us/step - loss: 2689362531.6232 - mse: 2689362432.0000 - val_loss: 220134542.9412 - val_mse: 220134544.0000\n",
      "Epoch 14/200\n",
      "272/272 [==============================] - 0s 754us/step - loss: 663533339.0331 - mse: 663533312.0000 - val_loss: 240166271.9412 - val_mse: 240166272.0000\n",
      "Epoch 15/200\n",
      "272/272 [==============================] - 0s 768us/step - loss: 761491292.1719 - mse: 761491264.0000 - val_loss: 273077869.1176 - val_mse: 273077856.0000\n",
      "Epoch 16/200\n",
      "272/272 [==============================] - 0s 724us/step - loss: 584353905.4007 - mse: 584353920.0000 - val_loss: 100997274.0147 - val_mse: 100997272.0000\n",
      "Epoch 17/200\n",
      "272/272 [==============================] - 0s 807us/step - loss: 379099054.7518 - mse: 379099200.0000 - val_loss: 85800631.2794 - val_mse: 85800640.0000\n",
      "Epoch 18/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 498949040.5938 - mse: 498949024.0000 - val_loss: 83148278.7426 - val_mse: 83148272.0000\n",
      "Epoch 19/200\n",
      "272/272 [==============================] - 0s 952us/step - loss: 1011697174.8254 - mse: 1011696960.0000 - val_loss: 145230822.4081 - val_mse: 145230800.0000\n",
      "Epoch 20/200\n",
      "272/272 [==============================] - 0s 812us/step - loss: 993117215.1507 - mse: 993117184.0000 - val_loss: 68561472.5000 - val_mse: 68561472.0000\n",
      "Epoch 21/200\n",
      "272/272 [==============================] - 0s 778us/step - loss: 328234880.1296 - mse: 328234816.0000 - val_loss: 193712599.2353 - val_mse: 193712608.0000\n",
      "Epoch 22/200\n",
      "272/272 [==============================] - 0s 691us/step - loss: 1019396023.4798 - mse: 1019396224.0000 - val_loss: 53894840.9154 - val_mse: 53894844.0000\n",
      "Epoch 23/200\n",
      "272/272 [==============================] - 0s 756us/step - loss: 1885293692.1765 - mse: 1885293696.0000 - val_loss: 48128115.8015 - val_mse: 48128116.0000\n",
      "Epoch 24/200\n",
      "272/272 [==============================] - 0s 798us/step - loss: 1055331796.3695 - mse: 1055331840.0000 - val_loss: 40263934.0000 - val_mse: 40263936.0000\n",
      "Epoch 25/200\n",
      "272/272 [==============================] - 0s 743us/step - loss: 1040982258.0331 - mse: 1040982272.0000 - val_loss: 797030783.0588 - val_mse: 797030848.0000\n",
      "Epoch 26/200\n",
      "272/272 [==============================] - 0s 690us/step - loss: 701520737.5423 - mse: 701520704.0000 - val_loss: 7786744967.5294 - val_mse: 7786744320.0000\n",
      "Epoch 27/200\n",
      "272/272 [==============================] - 0s 772us/step - loss: 1981476908.7132 - mse: 1981476864.0000 - val_loss: 814649576.7059 - val_mse: 814649600.0000\n",
      "Epoch 28/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 14989166886.0740 - mse: 14989169664.0000 - val_loss: 54634180.2858 - val_mse: 54634184.0000\n",
      "Epoch 29/200\n",
      "272/272 [==============================] - 0s 921us/step - loss: 685368295.8024 - mse: 685368256.0000 - val_loss: 1228990714.3529 - val_mse: 1228990720.0000\n",
      "Epoch 30/200\n",
      "272/272 [==============================] - 0s 807us/step - loss: 689817972.9283 - mse: 689817920.0000 - val_loss: 5236603619.7647 - val_mse: 5236603904.0000\n",
      "Epoch 31/200\n",
      "272/272 [==============================] - 0s 790us/step - loss: 678365463.9108 - mse: 678365440.0000 - val_loss: 76614218.7647 - val_mse: 76614208.0000\n",
      "Epoch 32/200\n",
      "272/272 [==============================] - 0s 872us/step - loss: 97117277.9494 - mse: 97117272.0000 - val_loss: 609918790.1176 - val_mse: 609918784.0000\n",
      "Epoch 33/200\n",
      "272/272 [==============================] - 0s 918us/step - loss: 1898742585.6581 - mse: 1898742528.0000 - val_loss: 36842069.3235 - val_mse: 36842068.0000\n",
      "Epoch 34/200\n",
      "272/272 [==============================] - 0s 774us/step - loss: 2854070939.4465 - mse: 2854070528.0000 - val_loss: 136420841.2279 - val_mse: 136420848.0000\n",
      "Epoch 35/200\n",
      "272/272 [==============================] - 0s 917us/step - loss: 362695613.7063 - mse: 362695552.0000 - val_loss: 3576664022.5882 - val_mse: 3576664064.0000\n",
      "Epoch 36/200\n",
      "272/272 [==============================] - 0s 761us/step - loss: 1390565272.5836 - mse: 1390565376.0000 - val_loss: 6042102.0653 - val_mse: 6042101.0000\n",
      "Epoch 37/200\n",
      "272/272 [==============================] - 0s 752us/step - loss: 42353713.2368 - mse: 42353716.0000 - val_loss: 108958028.9118 - val_mse: 108958024.0000\n",
      "Epoch 38/200\n",
      "272/272 [==============================] - 0s 739us/step - loss: 326629212.1994 - mse: 326629312.0000 - val_loss: 24198618.1213 - val_mse: 24198620.0000\n",
      "Epoch 39/200\n",
      "272/272 [==============================] - 0s 703us/step - loss: 79359052.9656 - mse: 79359056.0000 - val_loss: 84719634.8676 - val_mse: 84719632.0000\n",
      "Epoch 40/200\n",
      "272/272 [==============================] - 0s 725us/step - loss: 66476330.9933 - mse: 66476324.0000 - val_loss: 54993517.2941 - val_mse: 54993516.0000\n",
      "Epoch 41/200\n",
      "272/272 [==============================] - 0s 721us/step - loss: 37669280.8495 - mse: 37669284.0000 - val_loss: 87482226.9412 - val_mse: 87482224.0000\n",
      "Epoch 42/200\n",
      "272/272 [==============================] - 0s 815us/step - loss: 106641857.6675 - mse: 106641840.0000 - val_loss: 1543688.4696 - val_mse: 1543688.3750\n",
      "Epoch 43/200\n",
      "272/272 [==============================] - 0s 724us/step - loss: 77380018.5931 - mse: 77380032.0000 - val_loss: 190797106.2353 - val_mse: 190797088.0000\n",
      "Epoch 44/200\n",
      "272/272 [==============================] - 0s 893us/step - loss: 120732173.7229 - mse: 120732184.0000 - val_loss: 4814720.5508 - val_mse: 4814720.5000\n",
      "Epoch 45/200\n",
      "272/272 [==============================] - 0s 836us/step - loss: 79250792.8108 - mse: 79250800.0000 - val_loss: 557483.1137 - val_mse: 557483.1875\n",
      "Epoch 46/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 43731461.2304 - mse: 43731452.0000 - val_loss: 8739916.8199 - val_mse: 8739916.0000\n",
      "Epoch 47/200\n",
      "272/272 [==============================] - 0s 2ms/step - loss: 134578564.6461 - mse: 134578560.0000 - val_loss: 159742809.1765 - val_mse: 159742832.0000\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 767us/step - loss: 139544368.4845 - mse: 139544384.0000 - val_loss: 1406554.2470 - val_mse: 1406554.3750\n",
      "Epoch 49/200\n",
      "272/272 [==============================] - 0s 691us/step - loss: 2981733.3067 - mse: 2981733.0000 - val_loss: 145228273.6471 - val_mse: 145228272.0000\n",
      "Epoch 50/200\n",
      "272/272 [==============================] - 0s 791us/step - loss: 1074503834.3686 - mse: 1074503808.0000 - val_loss: 39336952.5294 - val_mse: 39336956.0000\n",
      "Epoch 51/200\n",
      "272/272 [==============================] - 0s 796us/step - loss: 89421987.9381 - mse: 89421992.0000 - val_loss: 141857856.0588 - val_mse: 141857856.0000\n",
      "Epoch 52/200\n",
      "272/272 [==============================] - 0s 686us/step - loss: 415559229.0332 - mse: 415559360.0000 - val_loss: 138194484.4118 - val_mse: 138194496.0000\n",
      "Epoch 53/200\n",
      "272/272 [==============================] - 0s 841us/step - loss: 94293760.4997 - mse: 94293752.0000 - val_loss: 148349133.4706 - val_mse: 148349136.0000\n",
      "Epoch 54/200\n",
      "272/272 [==============================] - 0s 748us/step - loss: 236620653.8352 - mse: 236620672.0000 - val_loss: 446105.4313 - val_mse: 446105.4375\n",
      "Epoch 55/200\n",
      "272/272 [==============================] - 0s 715us/step - loss: 12018849.7872 - mse: 12018850.0000 - val_loss: 1303998.2201 - val_mse: 1303998.2500\n",
      "Epoch 56/200\n",
      "272/272 [==============================] - 0s 760us/step - loss: 916294526.1978 - mse: 916294656.0000 - val_loss: 232659451.9412 - val_mse: 232659440.0000\n",
      "Epoch 57/200\n",
      "272/272 [==============================] - 0s 770us/step - loss: 1160063591.1982 - mse: 1160063744.0000 - val_loss: 406978932.2353 - val_mse: 406978912.0000\n",
      "Epoch 58/200\n",
      "272/272 [==============================] - 0s 745us/step - loss: 1714160936.5036 - mse: 1714160896.0000 - val_loss: 347294629.5294 - val_mse: 347294656.0000\n",
      "Epoch 59/200\n",
      "272/272 [==============================] - 0s 715us/step - loss: 403303288.8118 - mse: 403303232.0000 - val_loss: 60025491.6029 - val_mse: 60025492.0000\n",
      "Epoch 60/200\n",
      "272/272 [==============================] - 0s 754us/step - loss: 10578097011.3610 - mse: 10578097152.0000 - val_loss: 31701885.8529 - val_mse: 31701888.0000\n",
      "Epoch 61/200\n",
      "272/272 [==============================] - 0s 761us/step - loss: 2111123956.0064 - mse: 2111123968.0000 - val_loss: 10305081.0588 - val_mse: 10305081.0000\n",
      "Epoch 62/200\n",
      "272/272 [==============================] - 0s 763us/step - loss: 90911866.3107 - mse: 90911864.0000 - val_loss: 122157424.3529 - val_mse: 122157416.0000\n",
      "Epoch 63/200\n",
      "272/272 [==============================] - 0s 795us/step - loss: 67128294.1908 - mse: 67128296.0000 - val_loss: 94486448.5882 - val_mse: 94486448.0000\n",
      "Epoch 64/200\n",
      "272/272 [==============================] - 0s 850us/step - loss: 166699381.3093 - mse: 166699376.0000 - val_loss: 5809398.9992 - val_mse: 5809399.0000\n",
      "Epoch 65/200\n",
      "272/272 [==============================] - 0s 847us/step - loss: 73002816.5113 - mse: 73002816.0000 - val_loss: 22415133.8915 - val_mse: 22415134.0000\n",
      "Epoch 66/200\n",
      "272/272 [==============================] - 0s 733us/step - loss: 880474741.2965 - mse: 880474688.0000 - val_loss: 14826006377.4118 - val_mse: 14826006528.0000\n",
      "Epoch 67/200\n",
      "272/272 [==============================] - 0s 770us/step - loss: 1392546796.6550 - mse: 1392547456.0000 - val_loss: 118610468.2353 - val_mse: 118610472.0000\n",
      "Epoch 68/200\n",
      "272/272 [==============================] - 0s 834us/step - loss: 45915690.6197 - mse: 45915680.0000 - val_loss: 50644729.9412 - val_mse: 50644736.0000\n",
      "Epoch 69/200\n",
      "272/272 [==============================] - 0s 714us/step - loss: 2129659567.1471 - mse: 2129659264.0000 - val_loss: 880397884.2353 - val_mse: 880397888.0000\n",
      "Epoch 70/200\n",
      "272/272 [==============================] - 0s 838us/step - loss: 1742707464.5094 - mse: 1742707328.0000 - val_loss: 8709529.5432 - val_mse: 8709530.0000\n",
      "Epoch 71/200\n",
      "272/272 [==============================] - 0s 746us/step - loss: 34781466.2901 - mse: 34781468.0000 - val_loss: 9427098.0438 - val_mse: 9427097.0000\n",
      "Epoch 72/200\n",
      "272/272 [==============================] - 0s 741us/step - loss: 33099561.9665 - mse: 33099568.0000 - val_loss: 11047102.0142 - val_mse: 11047104.0000\n",
      "Epoch 73/200\n",
      "272/272 [==============================] - 0s 764us/step - loss: 1322873806.9854 - mse: 1322873856.0000 - val_loss: 1227779382.5882 - val_mse: 1227779072.0000\n",
      "Epoch 74/200\n",
      "272/272 [==============================] - 0s 706us/step - loss: 5497979198.9122 - mse: 5497978880.0000 - val_loss: 66350690.8015 - val_mse: 66350688.0000\n",
      "Epoch 75/200\n",
      "272/272 [==============================] - 0s 821us/step - loss: 72765815.9619 - mse: 72765776.0000 - val_loss: 17861322.9724 - val_mse: 17861324.0000\n",
      "Epoch 76/200\n",
      "272/272 [==============================] - 0s 787us/step - loss: 115920351.5047 - mse: 115920360.0000 - val_loss: 39834617.3824 - val_mse: 39834612.0000\n",
      "Epoch 77/200\n",
      "272/272 [==============================] - 0s 768us/step - loss: 185777240.1549 - mse: 185777264.0000 - val_loss: 85218265.4191 - val_mse: 85218272.0000\n",
      "Epoch 78/200\n",
      "272/272 [==============================] - 0s 805us/step - loss: 94653185.3824 - mse: 94653200.0000 - val_loss: 18076527.4412 - val_mse: 18076528.0000\n",
      "Epoch 79/200\n",
      "272/272 [==============================] - 0s 758us/step - loss: 106322199.1128 - mse: 106322200.0000 - val_loss: 28139869.1562 - val_mse: 28139870.0000\n",
      "Epoch 80/200\n",
      "272/272 [==============================] - 0s 804us/step - loss: 26139120.6742 - mse: 26139120.0000 - val_loss: 53981105.2353 - val_mse: 53981108.0000\n",
      "Epoch 81/200\n",
      "272/272 [==============================] - 0s 754us/step - loss: 50432579.5720 - mse: 50432580.0000 - val_loss: 22997215.6765 - val_mse: 22997214.0000\n",
      "Epoch 82/200\n",
      "272/272 [==============================] - 0s 820us/step - loss: 9773986.5796 - mse: 9773986.0000 - val_loss: 3791885.0202 - val_mse: 3791885.0000\n",
      "Epoch 83/200\n",
      "272/272 [==============================] - 0s 944us/step - loss: 19248213.7678 - mse: 19248218.0000 - val_loss: 4487898.8359 - val_mse: 4487899.0000\n",
      "Epoch 84/200\n",
      "272/272 [==============================] - 0s 952us/step - loss: 70259020.4180 - mse: 70259008.0000 - val_loss: 14828109.4412 - val_mse: 14828111.0000\n",
      "Epoch 85/200\n",
      "272/272 [==============================] - 0s 956us/step - loss: 181699869.9668 - mse: 181699824.0000 - val_loss: 6462870.0198 - val_mse: 6462870.0000\n",
      "Epoch 86/200\n",
      "272/272 [==============================] - 0s 859us/step - loss: 92964580.4893 - mse: 92964584.0000 - val_loss: 24247450.3493 - val_mse: 24247448.0000\n",
      "Epoch 87/200\n",
      "272/272 [==============================] - 0s 741us/step - loss: 30287268.4050 - mse: 30287266.0000 - val_loss: 177952746.5882 - val_mse: 177952768.0000\n",
      "Epoch 88/200\n",
      "272/272 [==============================] - 0s 818us/step - loss: 56577958.4840 - mse: 56577964.0000 - val_loss: 10049543.8088 - val_mse: 10049544.0000\n",
      "Epoch 89/200\n",
      "272/272 [==============================] - 0s 805us/step - loss: 44585160.3166 - mse: 44585160.0000 - val_loss: 542485761.8824 - val_mse: 542485824.0000\n",
      "Epoch 90/200\n",
      "272/272 [==============================] - 0s 757us/step - loss: 3625979399.0678 - mse: 3625979136.0000 - val_loss: 7982651.7042 - val_mse: 7982652.0000\n",
      "Epoch 91/200\n",
      "272/272 [==============================] - 0s 895us/step - loss: 10577963.3424 - mse: 10577966.0000 - val_loss: 2679338.2804 - val_mse: 2679338.5000\n",
      "Epoch 92/200\n",
      "272/272 [==============================] - 0s 792us/step - loss: 22812073.9847 - mse: 22812072.0000 - val_loss: 1443816.7835 - val_mse: 1443816.7500\n",
      "Epoch 93/200\n",
      "272/272 [==============================] - 0s 897us/step - loss: 15290201.8643 - mse: 15290202.0000 - val_loss: 811833.2739 - val_mse: 811833.3750\n",
      "Epoch 94/200\n",
      "272/272 [==============================] - 0s 855us/step - loss: 8135778.3711 - mse: 8135777.0000 - val_loss: 628638.3053 - val_mse: 628638.2500\n",
      "Epoch 95/200\n",
      "272/272 [==============================] - 0s 788us/step - loss: 27106698.5845 - mse: 27106698.0000 - val_loss: 3140656.5239 - val_mse: 3140656.7500\n",
      "Epoch 96/200\n",
      "272/272 [==============================] - 0s 942us/step - loss: 9168056.1969 - mse: 9168056.0000 - val_loss: 14840883.2868 - val_mse: 14840883.0000\n",
      "Epoch 97/200\n",
      "272/272 [==============================] - 0s 747us/step - loss: 506997867.3743 - mse: 506997824.0000 - val_loss: 2340348.3536 - val_mse: 2340348.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/200\n",
      "272/272 [==============================] - 0s 740us/step - loss: 259546173.0299 - mse: 259546144.0000 - val_loss: 220560323.4706 - val_mse: 220560336.0000\n",
      "Epoch 99/200\n",
      "272/272 [==============================] - 0s 809us/step - loss: 12670510649.5303 - mse: 12670511104.0000 - val_loss: 210449286.3088 - val_mse: 210449280.0000\n",
      "Epoch 100/200\n",
      "272/272 [==============================] - 0s 724us/step - loss: 160218886.2169 - mse: 160218896.0000 - val_loss: 246898055.7206 - val_mse: 246898048.0000\n",
      "Epoch 101/200\n",
      "272/272 [==============================] - 0s 763us/step - loss: 1148049910.6029 - mse: 1148050048.0000 - val_loss: 29467468.6618 - val_mse: 29467472.0000\n",
      "Epoch 102/200\n",
      "272/272 [==============================] - 0s 826us/step - loss: 130067595.6558 - mse: 130067592.0000 - val_loss: 37144963.5257 - val_mse: 37144964.0000\n",
      "Epoch 103/200\n",
      "272/272 [==============================] - 0s 799us/step - loss: 178376785.7489 - mse: 178376784.0000 - val_loss: 250304961.8824 - val_mse: 250304976.0000\n",
      "Epoch 104/200\n",
      "272/272 [==============================] - 0s 867us/step - loss: 55964898.1472 - mse: 55964900.0000 - val_loss: 21712027.4062 - val_mse: 21712028.0000\n",
      "Epoch 105/200\n",
      "272/272 [==============================] - 0s 716us/step - loss: 90278598.7390 - mse: 90278592.0000 - val_loss: 42663369.8529 - val_mse: 42663372.0000\n",
      "Epoch 106/200\n",
      "272/272 [==============================] - 0s 765us/step - loss: 79216169.8217 - mse: 79216168.0000 - val_loss: 16941894.7445 - val_mse: 16941896.0000\n",
      "Epoch 107/200\n",
      "272/272 [==============================] - 0s 864us/step - loss: 481519604.4265 - mse: 481519552.0000 - val_loss: 16740297.9779 - val_mse: 16740297.0000\n",
      "Epoch 108/200\n",
      "272/272 [==============================] - 0s 785us/step - loss: 176646276.7498 - mse: 176646304.0000 - val_loss: 283358295.2941 - val_mse: 283358336.0000\n",
      "Epoch 109/200\n",
      "272/272 [==============================] - 0s 794us/step - loss: 112727706.0283 - mse: 112727680.0000 - val_loss: 6455585.4982 - val_mse: 6455585.5000\n",
      "Epoch 110/200\n",
      "272/272 [==============================] - 0s 705us/step - loss: 108593559.3194 - mse: 108593560.0000 - val_loss: 5351143.0962 - val_mse: 5351143.5000\n",
      "Epoch 111/200\n",
      "272/272 [==============================] - 0s 754us/step - loss: 144718411.8628 - mse: 144718432.0000 - val_loss: 41671200.0000 - val_mse: 41671208.0000\n",
      "Epoch 112/200\n",
      "272/272 [==============================] - 0s 911us/step - loss: 35056334.5422 - mse: 35056328.0000 - val_loss: 13537982.2528 - val_mse: 13537982.0000\n",
      "Epoch 113/200\n",
      "272/272 [==============================] - 0s 771us/step - loss: 41799903.0954 - mse: 41799904.0000 - val_loss: 80013654.4706 - val_mse: 80013664.0000\n",
      "Epoch 114/200\n",
      "272/272 [==============================] - 0s 750us/step - loss: 93030823.5895 - mse: 93030840.0000 - val_loss: 5087538.2224 - val_mse: 5087538.0000\n",
      "Epoch 115/200\n",
      "272/272 [==============================] - 0s 682us/step - loss: 19382837.9380 - mse: 19382836.0000 - val_loss: 3177316.0110 - val_mse: 3177316.5000\n",
      "Epoch 116/200\n",
      "272/272 [==============================] - 0s 745us/step - loss: 12052063.4784 - mse: 12052061.0000 - val_loss: 2175340.6002 - val_mse: 2175340.7500\n",
      "Epoch 117/200\n",
      "272/272 [==============================] - 0s 765us/step - loss: 4321062.5926 - mse: 4321062.5000 - val_loss: 888023.6758 - val_mse: 888023.6875\n",
      "Epoch 118/200\n",
      "272/272 [==============================] - 0s 717us/step - loss: 1952209.4370 - mse: 1952209.6250 - val_loss: 2759727.8842 - val_mse: 2759728.0000\n",
      "Epoch 119/200\n",
      "272/272 [==============================] - 0s 725us/step - loss: 2298196.1493 - mse: 2298196.2500 - val_loss: 891993.2308 - val_mse: 891993.2500\n",
      "Epoch 120/200\n",
      "272/272 [==============================] - 0s 702us/step - loss: 679442.3526 - mse: 679442.3750 - val_loss: 680237.9704 - val_mse: 680238.0000\n",
      "Epoch 121/200\n",
      "272/272 [==============================] - 0s 691us/step - loss: 9930712.0240 - mse: 9930711.0000 - val_loss: 2148640.1645 - val_mse: 2148640.2500\n",
      "Epoch 122/200\n",
      "272/272 [==============================] - 0s 708us/step - loss: 2040443.5851 - mse: 2040443.7500 - val_loss: 381695.7027 - val_mse: 381695.7188\n",
      "Epoch 123/200\n",
      "272/272 [==============================] - 0s 811us/step - loss: 5733890.7384 - mse: 5733890.5000 - val_loss: 3134920.7335 - val_mse: 3134920.7500\n",
      "Epoch 124/200\n",
      "272/272 [==============================] - 0s 742us/step - loss: 19681565.3656 - mse: 19681560.0000 - val_loss: 358179.7988 - val_mse: 358179.8125\n",
      "Epoch 125/200\n",
      "272/272 [==============================] - 0s 699us/step - loss: 4374521.4658 - mse: 4374523.0000 - val_loss: 95699.1905 - val_mse: 95699.1875\n",
      "Epoch 126/200\n",
      "272/272 [==============================] - 0s 692us/step - loss: 658379.6352 - mse: 658379.5625 - val_loss: 523836.4290 - val_mse: 523836.4062\n",
      "Epoch 127/200\n",
      "272/272 [==============================] - 0s 699us/step - loss: 3766421.2735 - mse: 3766422.0000 - val_loss: 1015874.1622 - val_mse: 1015874.2500\n",
      "Epoch 128/200\n",
      "272/272 [==============================] - 0s 698us/step - loss: 4422566.8817 - mse: 4422566.0000 - val_loss: 123724.1598 - val_mse: 123724.1641\n",
      "Epoch 129/200\n",
      "272/272 [==============================] - 0s 705us/step - loss: 832158.0513 - mse: 832157.9375 - val_loss: 289777.7242 - val_mse: 289777.7188\n",
      "Epoch 130/200\n",
      "272/272 [==============================] - 0s 668us/step - loss: 40516661.3936 - mse: 40516660.0000 - val_loss: 10550832.3566 - val_mse: 10550833.0000\n",
      "Epoch 131/200\n",
      "272/272 [==============================] - 0s 694us/step - loss: 66292184.4155 - mse: 66292192.0000 - val_loss: 65954047.4706 - val_mse: 65954048.0000\n",
      "Epoch 132/200\n",
      "272/272 [==============================] - 0s 741us/step - loss: 55286401.1413 - mse: 55286408.0000 - val_loss: 1227557538.8235 - val_mse: 1227557504.0000\n",
      "Epoch 133/200\n",
      "272/272 [==============================] - 0s 699us/step - loss: 6391754958.8529 - mse: 6391754240.0000 - val_loss: 152540353.0000 - val_mse: 152540336.0000\n",
      "Epoch 134/200\n",
      "272/272 [==============================] - 0s 728us/step - loss: 308144072.6158 - mse: 308144128.0000 - val_loss: 450983065.2353 - val_mse: 450983104.0000\n",
      "Epoch 135/200\n",
      "272/272 [==============================] - 0s 694us/step - loss: 535969031.3824 - mse: 535968960.0000 - val_loss: 124387413.0588 - val_mse: 124387416.0000\n",
      "Epoch 136/200\n",
      "272/272 [==============================] - 0s 722us/step - loss: 361560921.6985 - mse: 361560896.0000 - val_loss: 54515879.8199 - val_mse: 54515876.0000\n",
      "Epoch 137/200\n",
      "272/272 [==============================] - 0s 719us/step - loss: 160369002.5988 - mse: 160369072.0000 - val_loss: 27641762.2941 - val_mse: 27641766.0000\n",
      "Epoch 138/200\n",
      "272/272 [==============================] - 0s 710us/step - loss: 333885846.7114 - mse: 333885888.0000 - val_loss: 184493185.4118 - val_mse: 184493184.0000\n",
      "Epoch 139/200\n",
      "272/272 [==============================] - 0s 679us/step - loss: 434550062.1526 - mse: 434550016.0000 - val_loss: 139055665.8824 - val_mse: 139055680.0000\n",
      "Epoch 140/200\n",
      "272/272 [==============================] - 0s 701us/step - loss: 144390135.8575 - mse: 144390112.0000 - val_loss: 110088251.2941 - val_mse: 110088256.0000\n",
      "Epoch 141/200\n",
      "272/272 [==============================] - 0s 705us/step - loss: 158144339.5616 - mse: 158144368.0000 - val_loss: 264178843.7647 - val_mse: 264178832.0000\n",
      "Epoch 142/200\n",
      "272/272 [==============================] - 0s 716us/step - loss: 271727490.1641 - mse: 271727520.0000 - val_loss: 16792400.8346 - val_mse: 16792400.0000\n",
      "Epoch 143/200\n",
      "272/272 [==============================] - 0s 754us/step - loss: 25879286.5951 - mse: 25879288.0000 - val_loss: 30841259.3235 - val_mse: 30841256.0000\n",
      "Epoch 144/200\n",
      "272/272 [==============================] - 0s 730us/step - loss: 91156575.2860 - mse: 91156568.0000 - val_loss: 26432227.9118 - val_mse: 26432228.0000\n",
      "Epoch 145/200\n",
      "272/272 [==============================] - 0s 725us/step - loss: 13041308.4855 - mse: 13041311.0000 - val_loss: 4431546.7130 - val_mse: 4431547.0000\n",
      "Epoch 146/200\n",
      "272/272 [==============================] - 0s 929us/step - loss: 11514792.1062 - mse: 11514791.0000 - val_loss: 3632758.3851 - val_mse: 3632758.5000\n",
      "Epoch 147/200\n",
      "272/272 [==============================] - 0s 684us/step - loss: 6018305.4496 - mse: 6018305.0000 - val_loss: 1465766.5927 - val_mse: 1465766.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      "272/272 [==============================] - 0s 681us/step - loss: 33531466.6927 - mse: 33531468.0000 - val_loss: 3156177.9632 - val_mse: 3156178.0000\n",
      "Epoch 149/200\n",
      "272/272 [==============================] - 0s 671us/step - loss: 5008368.7984 - mse: 5008369.5000 - val_loss: 393477.9891 - val_mse: 393478.0000\n",
      "Epoch 150/200\n",
      "272/272 [==============================] - 0s 656us/step - loss: 1766697.6903 - mse: 1766698.1250 - val_loss: 787882.3470 - val_mse: 787882.3750\n",
      "Epoch 151/200\n",
      "272/272 [==============================] - 0s 645us/step - loss: 2472204.9641 - mse: 2472204.2500 - val_loss: 304775.8017 - val_mse: 304775.8125\n",
      "Epoch 152/200\n",
      "272/272 [==============================] - 0s 660us/step - loss: 326294.5907 - mse: 326294.6562 - val_loss: 129564.6403 - val_mse: 129564.6484\n",
      "Epoch 153/200\n",
      "272/272 [==============================] - 0s 654us/step - loss: 1698684.1092 - mse: 1698684.0000 - val_loss: 110126.3636 - val_mse: 110126.3672\n",
      "Epoch 154/200\n",
      "272/272 [==============================] - 0s 644us/step - loss: 984795.6229 - mse: 984795.7500 - val_loss: 186190.1423 - val_mse: 186190.1719\n",
      "Epoch 155/200\n",
      "272/272 [==============================] - 0s 649us/step - loss: 955239.4923 - mse: 955239.3750 - val_loss: 228179.9922 - val_mse: 228179.9688\n",
      "Epoch 156/200\n",
      "272/272 [==============================] - 0s 659us/step - loss: 352359.2083 - mse: 352359.2500 - val_loss: 731919.3155 - val_mse: 731919.4375\n",
      "Epoch 157/200\n",
      "272/272 [==============================] - 0s 669us/step - loss: 717256.5420 - mse: 717256.5625 - val_loss: 111095.3624 - val_mse: 111095.3672\n",
      "Epoch 158/200\n",
      "272/272 [==============================] - 0s 675us/step - loss: 1939799.6646 - mse: 1939799.7500 - val_loss: 574453.7351 - val_mse: 574453.6875\n",
      "Epoch 159/200\n",
      "272/272 [==============================] - 0s 671us/step - loss: 1475573.8291 - mse: 1475573.8750 - val_loss: 1312157.2063 - val_mse: 1312157.1250\n",
      "Epoch 160/200\n",
      "272/272 [==============================] - 0s 657us/step - loss: 2930092.9207 - mse: 2930093.2500 - val_loss: 91116.5038 - val_mse: 91116.5000\n",
      "Epoch 161/200\n",
      "272/272 [==============================] - 0s 659us/step - loss: 7236487.9532 - mse: 7236487.5000 - val_loss: 135577.4342 - val_mse: 135577.4219\n",
      "Epoch 162/200\n",
      "272/272 [==============================] - 0s 651us/step - loss: 955371.5633 - mse: 955371.6250 - val_loss: 394857.3106 - val_mse: 394857.2812\n",
      "Epoch 163/200\n",
      "272/272 [==============================] - 0s 664us/step - loss: 17442103.3940 - mse: 17442108.0000 - val_loss: 1208505.4177 - val_mse: 1208505.5000\n",
      "Epoch 164/200\n",
      "272/272 [==============================] - 0s 677us/step - loss: 4339036.7594 - mse: 4339037.5000 - val_loss: 164448.8514 - val_mse: 164448.8594\n",
      "Epoch 165/200\n",
      "272/272 [==============================] - 0s 652us/step - loss: 1019704.7266 - mse: 1019704.8125 - val_loss: 103667.5428 - val_mse: 103667.5391\n",
      "Epoch 166/200\n",
      "272/272 [==============================] - 0s 639us/step - loss: 2637709.1618 - mse: 2637709.5000 - val_loss: 4487839.3153 - val_mse: 4487839.0000\n",
      "Epoch 167/200\n",
      "272/272 [==============================] - 0s 648us/step - loss: 8935184.1795 - mse: 8935184.0000 - val_loss: 2994632.9141 - val_mse: 2994633.0000\n",
      "Epoch 168/200\n",
      "272/272 [==============================] - 0s 671us/step - loss: 2495525.2656 - mse: 2495525.2500 - val_loss: 113865.1779 - val_mse: 113865.1797\n",
      "Epoch 169/200\n",
      "272/272 [==============================] - 0s 666us/step - loss: 974133.3527 - mse: 974133.3750 - val_loss: 3916676.9881 - val_mse: 3916677.2500\n",
      "Epoch 170/200\n",
      "272/272 [==============================] - 0s 702us/step - loss: 391229948.8166 - mse: 391229920.0000 - val_loss: 242577688.3382 - val_mse: 242577680.0000\n",
      "Epoch 171/200\n",
      "272/272 [==============================] - 0s 675us/step - loss: 771714852.2831 - mse: 771714944.0000 - val_loss: 71200186.7574 - val_mse: 71200184.0000\n",
      "Epoch 172/200\n",
      "272/272 [==============================] - 0s 792us/step - loss: 51650107.2491 - mse: 51650108.0000 - val_loss: 13364228.2868 - val_mse: 13364228.0000\n",
      "Epoch 173/200\n",
      "272/272 [==============================] - 0s 682us/step - loss: 41222664.3392 - mse: 41222672.0000 - val_loss: 14732006.9853 - val_mse: 14732008.0000\n",
      "Epoch 174/200\n",
      "272/272 [==============================] - 0s 766us/step - loss: 31241607.7829 - mse: 31241608.0000 - val_loss: 2144201.2307 - val_mse: 2144201.2500\n",
      "Epoch 175/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 75200900.1980 - mse: 75200928.0000 - val_loss: 2133230.3803 - val_mse: 2133230.2500\n",
      "Epoch 176/200\n",
      "272/272 [==============================] - 0s 676us/step - loss: 22543530.9922 - mse: 22543530.0000 - val_loss: 87211873.2059 - val_mse: 87211880.0000\n",
      "Epoch 177/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 112760399.3750 - mse: 112760384.0000 - val_loss: 11552537.0404 - val_mse: 11552537.0000\n",
      "Epoch 178/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 313911811.9353 - mse: 313911808.0000 - val_loss: 182229992.4706 - val_mse: 182229984.0000\n",
      "Epoch 179/200\n",
      "272/272 [==============================] - 0s 716us/step - loss: 404878732.5308 - mse: 404878688.0000 - val_loss: 24031980.8235 - val_mse: 24031982.0000\n",
      "Epoch 180/200\n",
      "272/272 [==============================] - 0s 747us/step - loss: 16717296.0717 - mse: 16717297.0000 - val_loss: 66417112.5882 - val_mse: 66417116.0000\n",
      "Epoch 181/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 26389238.5400 - mse: 26389240.0000 - val_loss: 46854760.2647 - val_mse: 46854752.0000\n",
      "Epoch 182/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 3638919.2309 - mse: 3638919.7500 - val_loss: 113625.7938 - val_mse: 113625.7969\n",
      "Epoch 183/200\n",
      "272/272 [==============================] - 0s 797us/step - loss: 418593.3800 - mse: 418593.3750 - val_loss: 196223.0485 - val_mse: 196223.0625\n",
      "Epoch 184/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 445172.3794 - mse: 445172.3438 - val_loss: 1275007.5198 - val_mse: 1275007.5000\n",
      "Epoch 185/200\n",
      "272/272 [==============================] - 0s 705us/step - loss: 566513.2002 - mse: 566513.1250 - val_loss: 1963102.2688 - val_mse: 1963102.3750\n",
      "Epoch 186/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 560170.7228 - mse: 560170.8125 - val_loss: 102100.3321 - val_mse: 102100.3203\n",
      "Epoch 187/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 602331.4974 - mse: 602331.4375 - val_loss: 174953.7481 - val_mse: 174953.7500\n",
      "Epoch 188/200\n",
      "272/272 [==============================] - 0s 690us/step - loss: 695907.8006 - mse: 695907.7500 - val_loss: 100502.7942 - val_mse: 100502.7891\n",
      "Epoch 189/200\n",
      "272/272 [==============================] - 0s 758us/step - loss: 601168.5549 - mse: 601168.5625 - val_loss: 797031.0267 - val_mse: 797031.0000\n",
      "Epoch 190/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 6094202.2860 - mse: 6094202.0000 - val_loss: 1718074.7040 - val_mse: 1718074.6250\n",
      "Epoch 191/200\n",
      "272/272 [==============================] - 0s 699us/step - loss: 3076444.9725 - mse: 3076445.0000 - val_loss: 194426.1386 - val_mse: 194426.1406\n",
      "Epoch 192/200\n",
      "272/272 [==============================] - 0s 680us/step - loss: 282529.9215 - mse: 282529.9375 - val_loss: 101997.7892 - val_mse: 101997.7891\n",
      "Epoch 193/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 677746.2535 - mse: 677746.1875 - val_loss: 170544.3407 - val_mse: 170544.3438\n",
      "Epoch 194/200\n",
      "272/272 [==============================] - 0s 752us/step - loss: 1900657.8113 - mse: 1900657.8750 - val_loss: 1168460.8272 - val_mse: 1168460.8750\n",
      "Epoch 195/200\n",
      "272/272 [==============================] - 0s 693us/step - loss: 512529.2187 - mse: 512529.1875 - val_loss: 147920.3152 - val_mse: 147920.3281\n",
      "Epoch 196/200\n",
      "272/272 [==============================] - 0s 861us/step - loss: 4605916.2161 - mse: 4605916.0000 - val_loss: 7968269.3925 - val_mse: 7968269.0000\n",
      "Epoch 197/200\n",
      "272/272 [==============================] - 0s 904us/step - loss: 29012365.6903 - mse: 29012372.0000 - val_loss: 40907464.9118 - val_mse: 40907472.0000\n",
      "Epoch 198/200\n",
      "272/272 [==============================] - 0s 892us/step - loss: 19805328.0223 - mse: 19805334.0000 - val_loss: 8166273.3971 - val_mse: 8166274.0000\n",
      "Epoch 199/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 839us/step - loss: 2702492.3253 - mse: 2702491.5000 - val_loss: 860094.6185 - val_mse: 860094.6875\n",
      "Epoch 200/200\n",
      "272/272 [==============================] - 0s 805us/step - loss: 2260270.2749 - mse: 2260270.0000 - val_loss: 99124.1763 - val_mse: 99124.1797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a3ced78d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습하기\n",
    "model.compile(loss = 'mse', optimizer='adam', metrics=['mse']) \n",
    "model.fit(x_train, y_train, epochs = 200, batch_size=3, validation_data=(x_val, y_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 1ms/step\n",
      "mse: 197428.109375\n",
      "loss: 197428.14584581243\n"
     ]
    }
   ],
   "source": [
    "#평가하기\n",
    "loss, mse = model.evaluate(x_test,y_test,batch_size=1)\n",
    "print('mse:', mse)\n",
    "print('loss:',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[57411.72]]\n"
     ]
    }
   ],
   "source": [
    "#예측하기\n",
    "x_prd = np.array([[57100, 58800, 56800, 23995260]])\n",
    "test = model.predict(x_prd, batch_size=4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prd = model.predict(x_test, batch_size = 1) # <------- 이거 중요~! x_test와 y_test 매치가 중요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  444.3288706015061\n"
     ]
    }
   ],
   "source": [
    "#RMSE 구하기\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def RMSE(y_test, y_predict):\n",
    "    return np.sqrt(mean_squared_error(y_test, y_predict)) \n",
    "\n",
    "print('RMSE : ', RMSE(y_test, y_prd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
