{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 불러오기\n",
    "df = pd.read_csv('./data/samsung.csv', encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>57,800</td>\n",
       "      <td>58,400</td>\n",
       "      <td>56,400</td>\n",
       "      <td>56,400</td>\n",
       "      <td>19,749,457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>58,800</td>\n",
       "      <td>58,800</td>\n",
       "      <td>56,800</td>\n",
       "      <td>57,200</td>\n",
       "      <td>20,821,939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>59,100</td>\n",
       "      <td>59,700</td>\n",
       "      <td>58,800</td>\n",
       "      <td>59,100</td>\n",
       "      <td>16,446,102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>59,400</td>\n",
       "      <td>59,400</td>\n",
       "      <td>58,300</td>\n",
       "      <td>58,800</td>\n",
       "      <td>23,664,541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>61,800</td>\n",
       "      <td>61,800</td>\n",
       "      <td>60,700</td>\n",
       "      <td>60,800</td>\n",
       "      <td>14,916,555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           일자      시가      고가      저가      종가         거래량\n",
       "0  2020-01-31  57,800  58,400  56,400  56,400  19,749,457\n",
       "1  2020-01-30  58,800  58,800  56,800  57,200  20,821,939\n",
       "2  2020-01-29  59,100  59,700  58,800  59,100  16,446,102\n",
       "3  2020-01-28  59,400  59,400  58,300  58,800  23,664,541\n",
       "4  2020-01-23  61,800  61,800  60,700  60,800  14,916,555"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터의 구성을 확인하기 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>2018-05-11</td>\n",
       "      <td>52,000</td>\n",
       "      <td>52,200</td>\n",
       "      <td>51,200</td>\n",
       "      <td>51,300</td>\n",
       "      <td>10,314,997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>2018-05-10</td>\n",
       "      <td>51,700</td>\n",
       "      <td>51,700</td>\n",
       "      <td>50,600</td>\n",
       "      <td>51,600</td>\n",
       "      <td>13,905,263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>2018-05-09</td>\n",
       "      <td>52,600</td>\n",
       "      <td>52,800</td>\n",
       "      <td>50,900</td>\n",
       "      <td>50,900</td>\n",
       "      <td>16,128,305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>52,600</td>\n",
       "      <td>53,200</td>\n",
       "      <td>51,900</td>\n",
       "      <td>52,600</td>\n",
       "      <td>23,104,720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>53,000</td>\n",
       "      <td>53,900</td>\n",
       "      <td>51,800</td>\n",
       "      <td>51,900</td>\n",
       "      <td>39,565,391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             일자      시가      고가      저가      종가         거래량\n",
       "421  2018-05-11  52,000  52,200  51,200  51,300  10,314,997\n",
       "422  2018-05-10  51,700  51,700  50,600  51,600  13,905,263\n",
       "423  2018-05-09  52,600  52,800  50,900  50,900  16,128,305\n",
       "424  2018-05-08  52,600  53,200  51,900  52,600  23,104,720\n",
       "425  2018-05-04  53,000  53,900  51,800  51,900  39,565,391"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터가 얼마나 있는지 확인하기\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#날짜를 기준으로 데이터 정렬해주기\n",
    "df=df.sort_values(['일자'], ascending = [True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 값들이 전부 문자열이므로 숫자들로 바꿔주기\n",
    "for col in df.columns[1:]:  # Iterate over chosen columns\n",
    "    df[col] = df[col].str.replace(\",\",\"\")\n",
    "    df[col] = pd.to_numeric(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size = 0.2, shuffle  = False) #train : 298행  / test : 128행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(train, test_size = 0.2, shuffle  = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>53000</td>\n",
       "      <td>53900</td>\n",
       "      <td>51800</td>\n",
       "      <td>51900</td>\n",
       "      <td>39565391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>52600</td>\n",
       "      <td>53200</td>\n",
       "      <td>51900</td>\n",
       "      <td>52600</td>\n",
       "      <td>23104720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>2018-05-09</td>\n",
       "      <td>52600</td>\n",
       "      <td>52800</td>\n",
       "      <td>50900</td>\n",
       "      <td>50900</td>\n",
       "      <td>16128305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>2018-05-10</td>\n",
       "      <td>51700</td>\n",
       "      <td>51700</td>\n",
       "      <td>50600</td>\n",
       "      <td>51600</td>\n",
       "      <td>13905263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>2018-05-11</td>\n",
       "      <td>52000</td>\n",
       "      <td>52200</td>\n",
       "      <td>51200</td>\n",
       "      <td>51300</td>\n",
       "      <td>10314997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             일자     시가     고가     저가     종가       거래량\n",
       "425  2018-05-04  53000  53900  51800  51900  39565391\n",
       "424  2018-05-08  52600  53200  51900  52600  23104720\n",
       "423  2018-05-09  52600  52800  50900  50900  16128305\n",
       "422  2018-05-10  51700  51700  50600  51600  13905263\n",
       "421  2018-05-11  52000  52200  51200  51300  10314997"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#일자를 기준으로 순서 정렬\n",
    "train = train.sort_values(['일자'], ascending = [True])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'일자'열은 학습에 필요없으니 지워주기\n",
    "del train['일자']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>53000</td>\n",
       "      <td>53900</td>\n",
       "      <td>51800</td>\n",
       "      <td>51900</td>\n",
       "      <td>39565391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>52600</td>\n",
       "      <td>53200</td>\n",
       "      <td>51900</td>\n",
       "      <td>52600</td>\n",
       "      <td>23104720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>52600</td>\n",
       "      <td>52800</td>\n",
       "      <td>50900</td>\n",
       "      <td>50900</td>\n",
       "      <td>16128305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>51700</td>\n",
       "      <td>51700</td>\n",
       "      <td>50600</td>\n",
       "      <td>51600</td>\n",
       "      <td>13905263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>52000</td>\n",
       "      <td>52200</td>\n",
       "      <td>51200</td>\n",
       "      <td>51300</td>\n",
       "      <td>10314997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>44800</td>\n",
       "      <td>45000</td>\n",
       "      <td>44550</td>\n",
       "      <td>44850</td>\n",
       "      <td>6664872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>44800</td>\n",
       "      <td>45050</td>\n",
       "      <td>44300</td>\n",
       "      <td>44600</td>\n",
       "      <td>8607439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>44200</td>\n",
       "      <td>44400</td>\n",
       "      <td>43400</td>\n",
       "      <td>43750</td>\n",
       "      <td>16906541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>43750</td>\n",
       "      <td>44150</td>\n",
       "      <td>43300</td>\n",
       "      <td>44000</td>\n",
       "      <td>9322873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>43750</td>\n",
       "      <td>44050</td>\n",
       "      <td>43400</td>\n",
       "      <td>43900</td>\n",
       "      <td>11890424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        시가     고가     저가     종가       거래량\n",
       "425  53000  53900  51800  51900  39565391\n",
       "424  52600  53200  51900  52600  23104720\n",
       "423  52600  52800  50900  50900  16128305\n",
       "422  51700  51700  50600  51600  13905263\n",
       "421  52000  52200  51200  51300  10314997\n",
       "..     ...    ...    ...    ...       ...\n",
       "158  44800  45000  44550  44850   6664872\n",
       "157  44800  45050  44300  44600   8607439\n",
       "156  44200  44400  43400  43750  16906541\n",
       "155  43750  44150  43300  44000   9322873\n",
       "154  43750  44050  43400  43900  11890424\n",
       "\n",
       "[272 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'일자'가 사라졌는지 확인하기\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train에서 학습용 y_train 분리\n",
    "y_train = train['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train에서 학습용 y_train 분리한 다음 '종가'열 지워주기\n",
    "del train['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train에서 종가열이 사라졌으니 x_train으로 이름 변경\n",
    "x_train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation 데이터도 마찬가지\n",
    "y_val = validation['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del validation['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_val['일자']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2019-09-25</td>\n",
       "      <td>49200</td>\n",
       "      <td>49350</td>\n",
       "      <td>48800</td>\n",
       "      <td>48900</td>\n",
       "      <td>9187141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2019-09-26</td>\n",
       "      <td>49000</td>\n",
       "      <td>49250</td>\n",
       "      <td>48900</td>\n",
       "      <td>49200</td>\n",
       "      <td>8494756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>48000</td>\n",
       "      <td>48700</td>\n",
       "      <td>48000</td>\n",
       "      <td>48400</td>\n",
       "      <td>8048041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>48050</td>\n",
       "      <td>49250</td>\n",
       "      <td>47900</td>\n",
       "      <td>49050</td>\n",
       "      <td>9497119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>48900</td>\n",
       "      <td>49100</td>\n",
       "      <td>48650</td>\n",
       "      <td>48850</td>\n",
       "      <td>6206035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            일자     시가     고가     저가     종가      거래량\n",
       "85  2019-09-25  49200  49350  48800  48900  9187141\n",
       "84  2019-09-26  49000  49250  48900  49200  8494756\n",
       "83  2019-09-27  48000  48700  48000  48400  8048041\n",
       "82  2019-09-30  48050  49250  47900  49050  9497119\n",
       "81  2019-10-01  48900  49100  48650  48850  6206035"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#일자를 기준으로 순서 정렬\n",
    "test = test.sort_values(['일자'], ascending = [True])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_test['일자']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#Keras 모듈 불러오기\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 45        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 18        \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 410\n",
      "Trainable params: 410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#모델 구성하기\n",
    "model = Sequential()#---------------------모델 방식\n",
    "\n",
    "model.add(Dense(10, input_shape = (4,))) #-Hidden Layer 1\n",
    "model.add(Dense(15))#---------------------Hidden Layer 2\n",
    "model.add(Dense(8)) #---------------------Hidden Layer 3\n",
    "model.add(Dense(5)) #---------------------Hidden Layer 4\n",
    "model.add(Dense(3)) #---------------------Hidden Layer 5\n",
    "model.add(Dense(1)) #---------------------Output Layer\n",
    "\n",
    "model.summary() #-------------------------레이어 형태&파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 272 samples, validate on 68 samples\n",
      "Epoch 1/200\n",
      "272/272 [==============================] - 1s 4ms/step - loss: 771042883762.7352 - mse: 771042705408.0000 - val_loss: 3406492491.2941 - val_mse: 3406492160.0000\n",
      "Epoch 2/200\n",
      "272/272 [==============================] - 0s 723us/step - loss: 1377954421.2794 - mse: 1377954432.0000 - val_loss: 4362780397.1765 - val_mse: 4362780160.0000\n",
      "Epoch 3/200\n",
      "272/272 [==============================] - 0s 749us/step - loss: 2509172264.8676 - mse: 2509172736.0000 - val_loss: 580045123.0588 - val_mse: 580045056.0000\n",
      "Epoch 4/200\n",
      "272/272 [==============================] - 0s 784us/step - loss: 3562160493.9926 - mse: 3562160128.0000 - val_loss: 1666039240.4706 - val_mse: 1666039168.0000\n",
      "Epoch 5/200\n",
      "272/272 [==============================] - 0s 735us/step - loss: 3405079144.0515 - mse: 3405079808.0000 - val_loss: 437947302.1765 - val_mse: 437947264.0000\n",
      "Epoch 6/200\n",
      "272/272 [==============================] - 0s 740us/step - loss: 4035541042.0882 - mse: 4035540736.0000 - val_loss: 429427626.4706 - val_mse: 429427648.0000\n",
      "Epoch 7/200\n",
      "272/272 [==============================] - 0s 753us/step - loss: 1761657062.6507 - mse: 1761657344.0000 - val_loss: 595366349.4706 - val_mse: 595366400.0000\n",
      "Epoch 8/200\n",
      "272/272 [==============================] - 0s 925us/step - loss: 1578986230.2941 - mse: 1578986112.0000 - val_loss: 1016506408.0882 - val_mse: 1016506368.0000\n",
      "Epoch 9/200\n",
      "272/272 [==============================] - 0s 768us/step - loss: 904747310.9412 - mse: 904747392.0000 - val_loss: 714978557.0000 - val_mse: 714978624.0000\n",
      "Epoch 10/200\n",
      "272/272 [==============================] - 0s 987us/step - loss: 1009495399.5294 - mse: 1009495424.0000 - val_loss: 580078649.0588 - val_mse: 580078720.0000\n",
      "Epoch 11/200\n",
      "272/272 [==============================] - 0s 959us/step - loss: 3624952255.6324 - mse: 3624952832.0000 - val_loss: 1769046879.1765 - val_mse: 1769047040.0000\n",
      "Epoch 12/200\n",
      "272/272 [==============================] - 0s 790us/step - loss: 1110981669.6176 - mse: 1110981376.0000 - val_loss: 357382729.2941 - val_mse: 357382784.0000\n",
      "Epoch 13/200\n",
      "272/272 [==============================] - 0s 753us/step - loss: 8067051290.4706 - mse: 8067050496.0000 - val_loss: 751870202.5882 - val_mse: 751870336.0000\n",
      "Epoch 14/200\n",
      "272/272 [==============================] - 0s 751us/step - loss: 2988067882.6324 - mse: 2988068352.0000 - val_loss: 332200959.4706 - val_mse: 332200960.0000\n",
      "Epoch 15/200\n",
      "272/272 [==============================] - 0s 754us/step - loss: 13475706827.4706 - mse: 13475707904.0000 - val_loss: 1750888998.2353 - val_mse: 1750889216.0000\n",
      "Epoch 16/200\n",
      "272/272 [==============================] - 0s 767us/step - loss: 2183433020.5588 - mse: 2183432960.0000 - val_loss: 412840138.6363 - val_mse: 412840128.0000\n",
      "Epoch 17/200\n",
      "272/272 [==============================] - 0s 692us/step - loss: 1014754095.5919 - mse: 1014753984.0000 - val_loss: 296989244.3015 - val_mse: 296989248.0000\n",
      "Epoch 18/200\n",
      "272/272 [==============================] - 0s 693us/step - loss: 1863163464.1324 - mse: 1863163520.0000 - val_loss: 466136234.8235 - val_mse: 466136224.0000\n",
      "Epoch 19/200\n",
      "272/272 [==============================] - 0s 729us/step - loss: 1159537658.3419 - mse: 1159537792.0000 - val_loss: 251435168.5882 - val_mse: 251435184.0000\n",
      "Epoch 20/200\n",
      "272/272 [==============================] - 0s 686us/step - loss: 4753767501.2574 - mse: 4753769472.0000 - val_loss: 799044371.7647 - val_mse: 799044288.0000\n",
      "Epoch 21/200\n",
      "272/272 [==============================] - 0s 741us/step - loss: 2912906563.2114 - mse: 2912907264.0000 - val_loss: 328409073.9890 - val_mse: 328409056.0000\n",
      "Epoch 22/200\n",
      "272/272 [==============================] - 0s 745us/step - loss: 1531134477.4265 - mse: 1531134336.0000 - val_loss: 2646853676.2353 - val_mse: 2646853632.0000\n",
      "Epoch 23/200\n",
      "272/272 [==============================] - 0s 735us/step - loss: 5811860974.9559 - mse: 5811859456.0000 - val_loss: 415896223.1176 - val_mse: 415896224.0000\n",
      "Epoch 24/200\n",
      "272/272 [==============================] - 0s 703us/step - loss: 676819835.7353 - mse: 676819968.0000 - val_loss: 854608831.0588 - val_mse: 854608832.0000\n",
      "Epoch 25/200\n",
      "272/272 [==============================] - 0s 731us/step - loss: 4491830950.3456 - mse: 4491830272.0000 - val_loss: 3556586333.1765 - val_mse: 3556585984.0000\n",
      "Epoch 26/200\n",
      "272/272 [==============================] - 0s 764us/step - loss: 5316763903.5000 - mse: 5316763648.0000 - val_loss: 459682070.9412 - val_mse: 459682048.0000\n",
      "Epoch 27/200\n",
      "272/272 [==============================] - 0s 708us/step - loss: 1345314249.5588 - mse: 1345314176.0000 - val_loss: 1613382226.8235 - val_mse: 1613382272.0000\n",
      "Epoch 28/200\n",
      "272/272 [==============================] - 0s 742us/step - loss: 1114384929.2022 - mse: 1114385024.0000 - val_loss: 267979845.3824 - val_mse: 267979872.0000\n",
      "Epoch 29/200\n",
      "272/272 [==============================] - 0s 756us/step - loss: 4028822712.4265 - mse: 4028822272.0000 - val_loss: 129880818.0294 - val_mse: 129880816.0000\n",
      "Epoch 30/200\n",
      "272/272 [==============================] - 0s 714us/step - loss: 1564946851.7544 - mse: 1564946944.0000 - val_loss: 496260916.2941 - val_mse: 496260896.0000\n",
      "Epoch 31/200\n",
      "272/272 [==============================] - 0s 774us/step - loss: 8579459246.9449 - mse: 8579459584.0000 - val_loss: 267360437.5294 - val_mse: 267360432.0000\n",
      "Epoch 32/200\n",
      "272/272 [==============================] - 0s 762us/step - loss: 2653593599.6250 - mse: 2653593856.0000 - val_loss: 138721447.9586 - val_mse: 138721440.0000\n",
      "Epoch 33/200\n",
      "272/272 [==============================] - 0s 767us/step - loss: 3010320827.8438 - mse: 3010320384.0000 - val_loss: 121235818.0441 - val_mse: 121235824.0000\n",
      "Epoch 34/200\n",
      "272/272 [==============================] - 0s 752us/step - loss: 1500258141.2518 - mse: 1500258048.0000 - val_loss: 2655545377.8824 - val_mse: 2655545344.0000\n",
      "Epoch 35/200\n",
      "272/272 [==============================] - 0s 697us/step - loss: 821286467.1710 - mse: 821286400.0000 - val_loss: 86253134.6618 - val_mse: 86253144.0000\n",
      "Epoch 36/200\n",
      "272/272 [==============================] - 0s 746us/step - loss: 4466261607.0074 - mse: 4466262016.0000 - val_loss: 1960064157.8824 - val_mse: 1960064128.0000\n",
      "Epoch 37/200\n",
      "272/272 [==============================] - 0s 695us/step - loss: 4545488845.7140 - mse: 4545487360.0000 - val_loss: 348762983.0588 - val_mse: 348762976.0000\n",
      "Epoch 38/200\n",
      "272/272 [==============================] - 0s 738us/step - loss: 197608264.6250 - mse: 197608256.0000 - val_loss: 248368165.0588 - val_mse: 248368176.0000\n",
      "Epoch 39/200\n",
      "272/272 [==============================] - 0s 825us/step - loss: 2655093549.0147 - mse: 2655095040.0000 - val_loss: 272962507.5294 - val_mse: 272962496.0000\n",
      "Epoch 40/200\n",
      "272/272 [==============================] - 0s 888us/step - loss: 179066792.7610 - mse: 179066784.0000 - val_loss: 1443210655.0588 - val_mse: 1443210496.0000\n",
      "Epoch 41/200\n",
      "272/272 [==============================] - 0s 953us/step - loss: 7259269135.7050 - mse: 7259271168.0000 - val_loss: 79872490.8233 - val_mse: 79872496.0000\n",
      "Epoch 42/200\n",
      "272/272 [==============================] - 0s 759us/step - loss: 177174663.5285 - mse: 177174656.0000 - val_loss: 181838652.4706 - val_mse: 181838656.0000\n",
      "Epoch 43/200\n",
      "272/272 [==============================] - 0s 692us/step - loss: 2918246945.5000 - mse: 2918246912.0000 - val_loss: 854707937.8824 - val_mse: 854707968.0000\n",
      "Epoch 44/200\n",
      "272/272 [==============================] - 0s 707us/step - loss: 395960564.8621 - mse: 395960480.0000 - val_loss: 203001522.5294 - val_mse: 203001520.0000\n",
      "Epoch 45/200\n",
      "272/272 [==============================] - 0s 723us/step - loss: 239931226.4770 - mse: 239931216.0000 - val_loss: 54479261.2647 - val_mse: 54479260.0000\n",
      "Epoch 46/200\n",
      "272/272 [==============================] - 0s 755us/step - loss: 1386679993.8888 - mse: 1386680064.0000 - val_loss: 150212953.2574 - val_mse: 150212944.0000\n",
      "Epoch 47/200\n",
      "272/272 [==============================] - 0s 697us/step - loss: 219688344.3153 - mse: 219688320.0000 - val_loss: 71972868.8676 - val_mse: 71972872.0000\n",
      "Epoch 48/200\n",
      "272/272 [==============================] - 0s 695us/step - loss: 436949144.0184 - mse: 436949088.0000 - val_loss: 39540389.0478 - val_mse: 39540384.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "272/272 [==============================] - 0s 684us/step - loss: 983946545.4099 - mse: 983946560.0000 - val_loss: 24893827.2169 - val_mse: 24893828.0000\n",
      "Epoch 50/200\n",
      "272/272 [==============================] - 0s 706us/step - loss: 380525268.6507 - mse: 380525184.0000 - val_loss: 84080825.9412 - val_mse: 84080824.0000\n",
      "Epoch 51/200\n",
      "272/272 [==============================] - 0s 664us/step - loss: 2478839681.1075 - mse: 2478839552.0000 - val_loss: 520828397.1765 - val_mse: 520828384.0000\n",
      "Epoch 52/200\n",
      "272/272 [==============================] - 0s 691us/step - loss: 1008499322.2013 - mse: 1008499200.0000 - val_loss: 1205288504.0000 - val_mse: 1205288320.0000\n",
      "Epoch 53/200\n",
      "272/272 [==============================] - 0s 697us/step - loss: 186159713.7911 - mse: 186159728.0000 - val_loss: 59466228.0588 - val_mse: 59466228.0000\n",
      "Epoch 54/200\n",
      "272/272 [==============================] - 0s 696us/step - loss: 1082019393.5699 - mse: 1082019200.0000 - val_loss: 1063032250.8235 - val_mse: 1063032320.0000\n",
      "Epoch 55/200\n",
      "272/272 [==============================] - 0s 772us/step - loss: 3124975077.1792 - mse: 3124974336.0000 - val_loss: 137071712.5074 - val_mse: 137071712.0000\n",
      "Epoch 56/200\n",
      "272/272 [==============================] - 0s 700us/step - loss: 87842880.0983 - mse: 87842888.0000 - val_loss: 35772596.0074 - val_mse: 35772588.0000\n",
      "Epoch 57/200\n",
      "272/272 [==============================] - 0s 704us/step - loss: 161489741.7693 - mse: 161489728.0000 - val_loss: 16154616.7334 - val_mse: 16154616.0000\n",
      "Epoch 58/200\n",
      "272/272 [==============================] - 0s 688us/step - loss: 162400515.6239 - mse: 162400496.0000 - val_loss: 35524741.5441 - val_mse: 35524744.0000\n",
      "Epoch 59/200\n",
      "272/272 [==============================] - 0s 687us/step - loss: 341672469.7302 - mse: 341672512.0000 - val_loss: 35240567.5294 - val_mse: 35240568.0000\n",
      "Epoch 60/200\n",
      "272/272 [==============================] - 0s 701us/step - loss: 1668917834.6648 - mse: 1668917760.0000 - val_loss: 2280965825.8824 - val_mse: 2280965888.0000\n",
      "Epoch 61/200\n",
      "272/272 [==============================] - 0s 717us/step - loss: 5730534632.6769 - mse: 5730534912.0000 - val_loss: 243174589.2941 - val_mse: 243174592.0000\n",
      "Epoch 62/200\n",
      "272/272 [==============================] - 0s 727us/step - loss: 363248057.4030 - mse: 363248064.0000 - val_loss: 152698470.4706 - val_mse: 152698480.0000\n",
      "Epoch 63/200\n",
      "272/272 [==============================] - 0s 703us/step - loss: 2403015699.7330 - mse: 2403016448.0000 - val_loss: 4734894.5450 - val_mse: 4734894.5000\n",
      "Epoch 64/200\n",
      "272/272 [==============================] - 0s 688us/step - loss: 77371973.4786 - mse: 77371968.0000 - val_loss: 5039224.7761 - val_mse: 5039224.5000\n",
      "Epoch 65/200\n",
      "272/272 [==============================] - 0s 746us/step - loss: 38157739.1669 - mse: 38157740.0000 - val_loss: 77549290.3456 - val_mse: 77549296.0000\n",
      "Epoch 66/200\n",
      "272/272 [==============================] - 0s 723us/step - loss: 43915753.1039 - mse: 43915748.0000 - val_loss: 5178904.6330 - val_mse: 5178904.5000\n",
      "Epoch 67/200\n",
      "272/272 [==============================] - 0s 712us/step - loss: 79362885.2343 - mse: 79362872.0000 - val_loss: 2354440.1075 - val_mse: 2354440.0000\n",
      "Epoch 68/200\n",
      "272/272 [==============================] - 0s 729us/step - loss: 435689916.5444 - mse: 435689952.0000 - val_loss: 8978665.4853 - val_mse: 8978665.0000\n",
      "Epoch 69/200\n",
      "272/272 [==============================] - 0s 678us/step - loss: 13957144.8398 - mse: 13957146.0000 - val_loss: 1891512.8695 - val_mse: 1891513.0000\n",
      "Epoch 70/200\n",
      "272/272 [==============================] - 0s 710us/step - loss: 21942642.2416 - mse: 21942642.0000 - val_loss: 2768273.8033 - val_mse: 2768273.2500\n",
      "Epoch 71/200\n",
      "272/272 [==============================] - 0s 738us/step - loss: 8017834.5202 - mse: 8017834.5000 - val_loss: 3038645.7300 - val_mse: 3038645.7500\n",
      "Epoch 72/200\n",
      "272/272 [==============================] - 0s 691us/step - loss: 15691434.3884 - mse: 15691433.0000 - val_loss: 455459079.2941 - val_mse: 455459072.0000\n",
      "Epoch 73/200\n",
      "272/272 [==============================] - 0s 729us/step - loss: 362949589.7239 - mse: 362949600.0000 - val_loss: 1570440.9447 - val_mse: 1570441.0000\n",
      "Epoch 74/200\n",
      "272/272 [==============================] - 0s 696us/step - loss: 238670797.7692 - mse: 238670784.0000 - val_loss: 6444055.6337 - val_mse: 6444055.0000\n",
      "Epoch 75/200\n",
      "272/272 [==============================] - 0s 697us/step - loss: 84562966.2376 - mse: 84562968.0000 - val_loss: 205277674.9412 - val_mse: 205277664.0000\n",
      "Epoch 76/200\n",
      "272/272 [==============================] - 0s 727us/step - loss: 1816484690.4112 - mse: 1816484352.0000 - val_loss: 13294150.9118 - val_mse: 13294151.0000\n",
      "Epoch 77/200\n",
      "272/272 [==============================] - 0s 700us/step - loss: 378852840.7183 - mse: 378852864.0000 - val_loss: 37553517.3235 - val_mse: 37553520.0000\n",
      "Epoch 78/200\n",
      "272/272 [==============================] - 0s 694us/step - loss: 2816919763.5611 - mse: 2816919296.0000 - val_loss: 751079039.2941 - val_mse: 751078976.0000\n",
      "Epoch 79/200\n",
      "272/272 [==============================] - 0s 711us/step - loss: 8497124403.0108 - mse: 8497122304.0000 - val_loss: 776537040.0000 - val_mse: 776536960.0000\n",
      "Epoch 80/200\n",
      "272/272 [==============================] - 0s 720us/step - loss: 852219431.0956 - mse: 852219456.0000 - val_loss: 25968346.9926 - val_mse: 25968346.0000\n",
      "Epoch 81/200\n",
      "272/272 [==============================] - 0s 714us/step - loss: 451908774.3061 - mse: 451908576.0000 - val_loss: 49474358.9081 - val_mse: 49474356.0000\n",
      "Epoch 82/200\n",
      "272/272 [==============================] - 0s 696us/step - loss: 117603035.8784 - mse: 117603064.0000 - val_loss: 7852100.7316 - val_mse: 7852101.5000\n",
      "Epoch 83/200\n",
      "272/272 [==============================] - 0s 728us/step - loss: 205437270.4804 - mse: 205437248.0000 - val_loss: 68193154.2610 - val_mse: 68193152.0000\n",
      "Epoch 84/200\n",
      "272/272 [==============================] - 0s 709us/step - loss: 1113328750.6591 - mse: 1113328384.0000 - val_loss: 15199028.7279 - val_mse: 15199028.0000\n",
      "Epoch 85/200\n",
      "272/272 [==============================] - 0s 692us/step - loss: 348246992.8286 - mse: 348246976.0000 - val_loss: 8372340.6381 - val_mse: 8372340.5000\n",
      "Epoch 86/200\n",
      "272/272 [==============================] - 0s 719us/step - loss: 276056945.5706 - mse: 276056960.0000 - val_loss: 200063091.8824 - val_mse: 200063104.0000\n",
      "Epoch 87/200\n",
      "272/272 [==============================] - 0s 698us/step - loss: 83102859.3914 - mse: 83102872.0000 - val_loss: 18398470.5028 - val_mse: 18398470.0000\n",
      "Epoch 88/200\n",
      "272/272 [==============================] - 0s 717us/step - loss: 270416716.3254 - mse: 270416704.0000 - val_loss: 6089434.7903 - val_mse: 6089435.0000\n",
      "Epoch 89/200\n",
      "272/272 [==============================] - 0s 685us/step - loss: 19183295.7788 - mse: 19183296.0000 - val_loss: 2769672.5918 - val_mse: 2769672.7500\n",
      "Epoch 90/200\n",
      "272/272 [==============================] - 0s 700us/step - loss: 680282058.5115 - mse: 680281920.0000 - val_loss: 10525783.0515 - val_mse: 10525784.0000\n",
      "Epoch 91/200\n",
      "272/272 [==============================] - 0s 923us/step - loss: 275424066.4014 - mse: 275424064.0000 - val_loss: 1279088.4697 - val_mse: 1279088.6250\n",
      "Epoch 92/200\n",
      "272/272 [==============================] - 0s 870us/step - loss: 51637953.9798 - mse: 51637948.0000 - val_loss: 112395530.4412 - val_mse: 112395536.0000\n",
      "Epoch 93/200\n",
      "272/272 [==============================] - 0s 949us/step - loss: 23360299.7143 - mse: 23360298.0000 - val_loss: 2513718.9210 - val_mse: 2513718.7500\n",
      "Epoch 94/200\n",
      "272/272 [==============================] - 0s 693us/step - loss: 89452421.3433 - mse: 89452440.0000 - val_loss: 36815400.0000 - val_mse: 36815404.0000\n",
      "Epoch 95/200\n",
      "272/272 [==============================] - 0s 716us/step - loss: 88672405.7362 - mse: 88672408.0000 - val_loss: 11815056.9412 - val_mse: 11815058.0000\n",
      "Epoch 96/200\n",
      "272/272 [==============================] - 0s 721us/step - loss: 429233445.8922 - mse: 429233376.0000 - val_loss: 787278.6859 - val_mse: 787278.7500\n",
      "Epoch 97/200\n",
      "272/272 [==============================] - 0s 692us/step - loss: 40950988.9704 - mse: 40950980.0000 - val_loss: 5983094.3566 - val_mse: 5983094.5000\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 702us/step - loss: 198602431.7942 - mse: 198602464.0000 - val_loss: 14588221.0294 - val_mse: 14588221.0000\n",
      "Epoch 99/200\n",
      "272/272 [==============================] - 0s 668us/step - loss: 209794158.3197 - mse: 209794208.0000 - val_loss: 509998.5041 - val_mse: 509998.5312\n",
      "Epoch 100/200\n",
      "272/272 [==============================] - 0s 730us/step - loss: 11889219.1336 - mse: 11889219.0000 - val_loss: 2303698.6438 - val_mse: 2303698.5000\n",
      "Epoch 101/200\n",
      "272/272 [==============================] - 0s 784us/step - loss: 29257643.8362 - mse: 29257644.0000 - val_loss: 3434230.2509 - val_mse: 3434230.2500\n",
      "Epoch 102/200\n",
      "272/272 [==============================] - 0s 742us/step - loss: 131716863.4988 - mse: 131716864.0000 - val_loss: 1319030940.7059 - val_mse: 1319030912.0000\n",
      "Epoch 103/200\n",
      "272/272 [==============================] - 0s 737us/step - loss: 25309766909.6121 - mse: 25309759488.0000 - val_loss: 78304521.2941 - val_mse: 78304520.0000\n",
      "Epoch 104/200\n",
      "272/272 [==============================] - 0s 849us/step - loss: 221570734.8277 - mse: 221570736.0000 - val_loss: 57288294.0294 - val_mse: 57288292.0000\n",
      "Epoch 105/200\n",
      "272/272 [==============================] - 0s 757us/step - loss: 429995597.4926 - mse: 429995584.0000 - val_loss: 65001564.8235 - val_mse: 65001564.0000\n",
      "Epoch 106/200\n",
      "272/272 [==============================] - 0s 705us/step - loss: 31609613.7316 - mse: 31609612.0000 - val_loss: 12759447.5437 - val_mse: 12759448.0000\n",
      "Epoch 107/200\n",
      "272/272 [==============================] - 0s 714us/step - loss: 78339761.5988 - mse: 78339752.0000 - val_loss: 21249055.0368 - val_mse: 21249056.0000\n",
      "Epoch 108/200\n",
      "272/272 [==============================] - 0s 993us/step - loss: 31323498.3000 - mse: 31323492.0000 - val_loss: 24831902.6471 - val_mse: 24831900.0000\n",
      "Epoch 109/200\n",
      "272/272 [==============================] - 0s 745us/step - loss: 32443740.2461 - mse: 32443738.0000 - val_loss: 123809982.5588 - val_mse: 123809984.0000\n",
      "Epoch 110/200\n",
      "272/272 [==============================] - 0s 721us/step - loss: 100361305.9228 - mse: 100361328.0000 - val_loss: 52946238.7647 - val_mse: 52946232.0000\n",
      "Epoch 111/200\n",
      "272/272 [==============================] - 0s 904us/step - loss: 65097576.4430 - mse: 65097584.0000 - val_loss: 25950710.5588 - val_mse: 25950712.0000\n",
      "Epoch 112/200\n",
      "272/272 [==============================] - 0s 786us/step - loss: 103045591.8807 - mse: 103045576.0000 - val_loss: 4099764.5000 - val_mse: 4099764.7500\n",
      "Epoch 113/200\n",
      "272/272 [==============================] - 0s 720us/step - loss: 99191695.9134 - mse: 99191680.0000 - val_loss: 3696840.5506 - val_mse: 3696840.5000\n",
      "Epoch 114/200\n",
      "272/272 [==============================] - 0s 790us/step - loss: 21163183.8215 - mse: 21163184.0000 - val_loss: 576437168.4706 - val_mse: 576437120.0000\n",
      "Epoch 115/200\n",
      "272/272 [==============================] - 0s 768us/step - loss: 69861458.1661 - mse: 69861464.0000 - val_loss: 3867170.0265 - val_mse: 3867170.0000\n",
      "Epoch 116/200\n",
      "272/272 [==============================] - 0s 697us/step - loss: 40387962.8954 - mse: 40387968.0000 - val_loss: 2251212.6673 - val_mse: 2251212.7500\n",
      "Epoch 117/200\n",
      "272/272 [==============================] - 0s 693us/step - loss: 59995424.8791 - mse: 59995424.0000 - val_loss: 10376413.5064 - val_mse: 10376413.0000\n",
      "Epoch 118/200\n",
      "272/272 [==============================] - 0s 661us/step - loss: 13974100.0865 - mse: 13974101.0000 - val_loss: 1886439.3975 - val_mse: 1886439.3750\n",
      "Epoch 119/200\n",
      "272/272 [==============================] - 0s 675us/step - loss: 29888833.1167 - mse: 29888832.0000 - val_loss: 1190181.1295 - val_mse: 1190181.0000\n",
      "Epoch 120/200\n",
      "272/272 [==============================] - 0s 706us/step - loss: 32225266.5218 - mse: 32225264.0000 - val_loss: 6429021.2243 - val_mse: 6429021.5000\n",
      "Epoch 121/200\n",
      "272/272 [==============================] - 0s 680us/step - loss: 3532711.7345 - mse: 3532711.5000 - val_loss: 885578.5536 - val_mse: 885578.5625\n",
      "Epoch 122/200\n",
      "272/272 [==============================] - 0s 714us/step - loss: 4646823.3055 - mse: 4646823.0000 - val_loss: 1051343.5916 - val_mse: 1051343.6250\n",
      "Epoch 123/200\n",
      "272/272 [==============================] - 0s 671us/step - loss: 4554593.7365 - mse: 4554594.5000 - val_loss: 607849.9760 - val_mse: 607849.9375\n",
      "Epoch 124/200\n",
      "272/272 [==============================] - 0s 687us/step - loss: 2193519.1508 - mse: 2193519.5000 - val_loss: 1556868.9421 - val_mse: 1556869.0000\n",
      "Epoch 125/200\n",
      "272/272 [==============================] - 0s 828us/step - loss: 2989455.2295 - mse: 2989454.7500 - val_loss: 10546163.8897 - val_mse: 10546163.0000\n",
      "Epoch 126/200\n",
      "272/272 [==============================] - 0s 696us/step - loss: 98640139.6115 - mse: 98640144.0000 - val_loss: 16190103.2353 - val_mse: 16190103.0000\n",
      "Epoch 127/200\n",
      "272/272 [==============================] - 0s 718us/step - loss: 36494469.5275 - mse: 36494476.0000 - val_loss: 2012675.9108 - val_mse: 2012675.7500\n",
      "Epoch 128/200\n",
      "272/272 [==============================] - 0s 742us/step - loss: 1873548.9406 - mse: 1873549.0000 - val_loss: 2043548.2096 - val_mse: 2043548.2500\n",
      "Epoch 129/200\n",
      "272/272 [==============================] - 0s 726us/step - loss: 17587421.7980 - mse: 17587420.0000 - val_loss: 282654.4200 - val_mse: 282654.4375\n",
      "Epoch 130/200\n",
      "272/272 [==============================] - 0s 720us/step - loss: 58613228.8808 - mse: 58613232.0000 - val_loss: 17829525.2059 - val_mse: 17829524.0000\n",
      "Epoch 131/200\n",
      "272/272 [==============================] - 0s 733us/step - loss: 9937343.0688 - mse: 9937338.0000 - val_loss: 434901.0761 - val_mse: 434901.0625\n",
      "Epoch 132/200\n",
      "272/272 [==============================] - 0s 723us/step - loss: 14674341.6122 - mse: 14674343.0000 - val_loss: 152358015.8824 - val_mse: 152358016.0000\n",
      "Epoch 133/200\n",
      "272/272 [==============================] - 0s 913us/step - loss: 28060588.0344 - mse: 28060584.0000 - val_loss: 15208539.5662 - val_mse: 15208539.0000\n",
      "Epoch 134/200\n",
      "272/272 [==============================] - 0s 707us/step - loss: 223788377.3943 - mse: 223788352.0000 - val_loss: 5799245.7050 - val_mse: 5799245.0000\n",
      "Epoch 135/200\n",
      "272/272 [==============================] - 0s 870us/step - loss: 6571744.1198 - mse: 6571741.0000 - val_loss: 604303.4945 - val_mse: 604303.5000\n",
      "Epoch 136/200\n",
      "272/272 [==============================] - 0s 794us/step - loss: 977466.8585 - mse: 977466.6250 - val_loss: 1684821.6884 - val_mse: 1684821.6250\n",
      "Epoch 137/200\n",
      "272/272 [==============================] - 0s 733us/step - loss: 2505539.6079 - mse: 2505539.2500 - val_loss: 15694788.9044 - val_mse: 15694788.0000\n",
      "Epoch 138/200\n",
      "272/272 [==============================] - 0s 842us/step - loss: 3677423.5776 - mse: 3677422.7500 - val_loss: 119319.5716 - val_mse: 119319.5781\n",
      "Epoch 139/200\n",
      "272/272 [==============================] - 0s 884us/step - loss: 25866858.0771 - mse: 25866858.0000 - val_loss: 6507494.0202 - val_mse: 6507493.5000\n",
      "Epoch 140/200\n",
      "272/272 [==============================] - 0s 733us/step - loss: 14551163.8135 - mse: 14551162.0000 - val_loss: 22809335.7426 - val_mse: 22809336.0000\n",
      "Epoch 141/200\n",
      "272/272 [==============================] - 0s 771us/step - loss: 35667782.0942 - mse: 35667784.0000 - val_loss: 330987156.8235 - val_mse: 330987136.0000\n",
      "Epoch 142/200\n",
      "272/272 [==============================] - 0s 906us/step - loss: 5552128274.1972 - mse: 5552128512.0000 - val_loss: 2063212956.2353 - val_mse: 2063212800.0000\n",
      "Epoch 143/200\n",
      "272/272 [==============================] - 0s 785us/step - loss: 296095306.3065 - mse: 296095296.0000 - val_loss: 3548324131.7647 - val_mse: 3548324352.0000\n",
      "Epoch 144/200\n",
      "272/272 [==============================] - 0s 733us/step - loss: 424919937.0616 - mse: 424920064.0000 - val_loss: 53903737.7059 - val_mse: 53903732.0000\n",
      "Epoch 145/200\n",
      "272/272 [==============================] - 0s 786us/step - loss: 115197329.8246 - mse: 115197304.0000 - val_loss: 81420745.2353 - val_mse: 81420744.0000\n",
      "Epoch 146/200\n",
      "272/272 [==============================] - 0s 823us/step - loss: 289638231.8157 - mse: 289638208.0000 - val_loss: 28720388.5809 - val_mse: 28720390.0000\n",
      "Epoch 147/200\n",
      "272/272 [==============================] - 0s 881us/step - loss: 117548515.9205 - mse: 117548504.0000 - val_loss: 167214178.8235 - val_mse: 167214160.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      "272/272 [==============================] - 0s 742us/step - loss: 535163013.6438 - mse: 535162880.0000 - val_loss: 91060773.1176 - val_mse: 91060768.0000\n",
      "Epoch 149/200\n",
      "272/272 [==============================] - 0s 765us/step - loss: 164969393.8640 - mse: 164969344.0000 - val_loss: 13948963.5993 - val_mse: 13948964.0000\n",
      "Epoch 150/200\n",
      "272/272 [==============================] - 0s 860us/step - loss: 155813761.8406 - mse: 155813808.0000 - val_loss: 126475212.3529 - val_mse: 126475216.0000\n",
      "Epoch 151/200\n",
      "272/272 [==============================] - 0s 715us/step - loss: 134341142.0517 - mse: 134341168.0000 - val_loss: 114657721.1765 - val_mse: 114657728.0000\n",
      "Epoch 152/200\n",
      "272/272 [==============================] - 0s 683us/step - loss: 248680775.6915 - mse: 248680800.0000 - val_loss: 48873943.1765 - val_mse: 48873948.0000\n",
      "Epoch 153/200\n",
      "272/272 [==============================] - 0s 681us/step - loss: 211268761.3061 - mse: 211268736.0000 - val_loss: 17526196.3640 - val_mse: 17526196.0000\n",
      "Epoch 154/200\n",
      "272/272 [==============================] - 0s 677us/step - loss: 78258122.2778 - mse: 78258112.0000 - val_loss: 59924768.1176 - val_mse: 59924768.0000\n",
      "Epoch 155/200\n",
      "272/272 [==============================] - 0s 692us/step - loss: 45009989.6382 - mse: 45009972.0000 - val_loss: 22898876.8824 - val_mse: 22898878.0000\n",
      "Epoch 156/200\n",
      "272/272 [==============================] - 0s 765us/step - loss: 78663955.1101 - mse: 78663968.0000 - val_loss: 22473016.7647 - val_mse: 22473018.0000\n",
      "Epoch 157/200\n",
      "272/272 [==============================] - 0s 700us/step - loss: 480253815.3637 - mse: 480253696.0000 - val_loss: 14277551.0513 - val_mse: 14277551.0000\n",
      "Epoch 158/200\n",
      "272/272 [==============================] - 0s 689us/step - loss: 262222139.7204 - mse: 262222128.0000 - val_loss: 11220022.3309 - val_mse: 11220023.0000\n",
      "Epoch 159/200\n",
      "272/272 [==============================] - 0s 713us/step - loss: 75221305.4720 - mse: 75221304.0000 - val_loss: 22914830.1954 - val_mse: 22914832.0000\n",
      "Epoch 160/200\n",
      "272/272 [==============================] - 0s 688us/step - loss: 521520952.4375 - mse: 521520736.0000 - val_loss: 42514519.4853 - val_mse: 42514520.0000\n",
      "Epoch 161/200\n",
      "272/272 [==============================] - 0s 788us/step - loss: 107735850.3211 - mse: 107735864.0000 - val_loss: 74857596.2941 - val_mse: 74857600.0000\n",
      "Epoch 162/200\n",
      "272/272 [==============================] - 0s 707us/step - loss: 100818772.7574 - mse: 100818760.0000 - val_loss: 14287218.6691 - val_mse: 14287218.0000\n",
      "Epoch 163/200\n",
      "272/272 [==============================] - 0s 692us/step - loss: 32385790.6101 - mse: 32385796.0000 - val_loss: 14615735.5441 - val_mse: 14615734.0000\n",
      "Epoch 164/200\n",
      "272/272 [==============================] - 0s 743us/step - loss: 53162956.9173 - mse: 53162952.0000 - val_loss: 17351867.1618 - val_mse: 17351864.0000\n",
      "Epoch 165/200\n",
      "272/272 [==============================] - 0s 743us/step - loss: 55696451.7574 - mse: 55696448.0000 - val_loss: 3266385.9954 - val_mse: 3266386.0000\n",
      "Epoch 166/200\n",
      "272/272 [==============================] - 0s 691us/step - loss: 20189200.8340 - mse: 20189200.0000 - val_loss: 1245354.1992 - val_mse: 1245354.3750\n",
      "Epoch 167/200\n",
      "272/272 [==============================] - 0s 662us/step - loss: 80460707.3617 - mse: 80460728.0000 - val_loss: 1570856.1985 - val_mse: 1570856.1250\n",
      "Epoch 168/200\n",
      "272/272 [==============================] - 0s 680us/step - loss: 5845434.7190 - mse: 5845435.5000 - val_loss: 4161334.1434 - val_mse: 4161334.5000\n",
      "Epoch 169/200\n",
      "272/272 [==============================] - 0s 704us/step - loss: 36793773.8631 - mse: 36793776.0000 - val_loss: 6399114.9412 - val_mse: 6399115.0000\n",
      "Epoch 170/200\n",
      "272/272 [==============================] - ETA: 0s - loss: 48502762.9738 - mse: 48502744.000 - 0s 941us/step - loss: 44425668.0270 - mse: 44425648.0000 - val_loss: 174936.1658 - val_mse: 174936.1562\n",
      "Epoch 171/200\n",
      "272/272 [==============================] - 0s 798us/step - loss: 425581.0627 - mse: 425581.0312 - val_loss: 408119.5685 - val_mse: 408119.5625\n",
      "Epoch 172/200\n",
      "272/272 [==============================] - 0s 836us/step - loss: 1038555.7100 - mse: 1038555.7500 - val_loss: 274032.1138 - val_mse: 274032.1250\n",
      "Epoch 173/200\n",
      "272/272 [==============================] - 0s 752us/step - loss: 10470367.9708 - mse: 10470367.0000 - val_loss: 9047486.7610 - val_mse: 9047487.0000\n",
      "Epoch 174/200\n",
      "272/272 [==============================] - 0s 811us/step - loss: 5032097.4212 - mse: 5032098.5000 - val_loss: 6196042.3015 - val_mse: 6196042.5000\n",
      "Epoch 175/200\n",
      "272/272 [==============================] - 0s 750us/step - loss: 8611628.2922 - mse: 8611629.0000 - val_loss: 6497838.7160 - val_mse: 6497838.0000\n",
      "Epoch 176/200\n",
      "272/272 [==============================] - 0s 823us/step - loss: 3355709.7600 - mse: 3355710.7500 - val_loss: 5417473.1912 - val_mse: 5417473.5000\n",
      "Epoch 177/200\n",
      "272/272 [==============================] - 0s 850us/step - loss: 315077391.0094 - mse: 315077408.0000 - val_loss: 12652773.4926 - val_mse: 12652775.0000\n",
      "Epoch 178/200\n",
      "272/272 [==============================] - 0s 906us/step - loss: 19429373.0659 - mse: 19429372.0000 - val_loss: 43013292.5000 - val_mse: 43013296.0000\n",
      "Epoch 179/200\n",
      "272/272 [==============================] - 0s 850us/step - loss: 58261024.0093 - mse: 58261024.0000 - val_loss: 502618376.2353 - val_mse: 502618368.0000\n",
      "Epoch 180/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 110080187.0011 - mse: 110080192.0000 - val_loss: 10124722.9853 - val_mse: 10124723.0000\n",
      "Epoch 181/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 38470902.3918 - mse: 38470892.0000 - val_loss: 1450875.3182 - val_mse: 1450875.2500\n",
      "Epoch 182/200\n",
      "272/272 [==============================] - 0s 1ms/step - loss: 7164114.0100 - mse: 7164112.0000 - val_loss: 633421.9956 - val_mse: 633422.0625\n",
      "Epoch 183/200\n",
      "272/272 [==============================] - 0s 903us/step - loss: 46690549.8966 - mse: 46690548.0000 - val_loss: 9259529.5910 - val_mse: 9259530.0000\n",
      "Epoch 184/200\n",
      "272/272 [==============================] - 0s 779us/step - loss: 9997343.5063 - mse: 9997342.0000 - val_loss: 27423328.9706 - val_mse: 27423328.0000\n",
      "Epoch 185/200\n",
      "272/272 [==============================] - 0s 748us/step - loss: 39972736.8009 - mse: 39972744.0000 - val_loss: 26431981.8456 - val_mse: 26431982.0000\n",
      "Epoch 186/200\n",
      "272/272 [==============================] - 0s 797us/step - loss: 8408666.8919 - mse: 8408667.0000 - val_loss: 6944045.8493 - val_mse: 6944046.0000\n",
      "Epoch 187/200\n",
      "272/272 [==============================] - 0s 798us/step - loss: 1129593.9860 - mse: 1129594.0000 - val_loss: 107110.5462 - val_mse: 107110.5469\n",
      "Epoch 188/200\n",
      "272/272 [==============================] - 0s 792us/step - loss: 1111191.1571 - mse: 1111191.0000 - val_loss: 1144367.4088 - val_mse: 1144367.5000\n",
      "Epoch 189/200\n",
      "272/272 [==============================] - 0s 877us/step - loss: 4779319.7022 - mse: 4779318.5000 - val_loss: 102311.3782 - val_mse: 102311.3750\n",
      "Epoch 190/200\n",
      "272/272 [==============================] - 0s 763us/step - loss: 9933048.9701 - mse: 9933048.0000 - val_loss: 1100031.1121 - val_mse: 1100031.1250\n",
      "Epoch 191/200\n",
      "272/272 [==============================] - 0s 783us/step - loss: 2417492.5225 - mse: 2417492.5000 - val_loss: 118441.9868 - val_mse: 118441.9844\n",
      "Epoch 192/200\n",
      "272/272 [==============================] - 0s 840us/step - loss: 2788483.1035 - mse: 2788483.2500 - val_loss: 620842.0331 - val_mse: 620842.0000\n",
      "Epoch 193/200\n",
      "272/272 [==============================] - 0s 763us/step - loss: 7041536.2458 - mse: 7041538.0000 - val_loss: 5109681.7169 - val_mse: 5109682.0000\n",
      "Epoch 194/200\n",
      "272/272 [==============================] - 0s 809us/step - loss: 29958850.5689 - mse: 29958850.0000 - val_loss: 3145269.9747 - val_mse: 3145270.0000\n",
      "Epoch 195/200\n",
      "272/272 [==============================] - 0s 782us/step - loss: 96239838.6084 - mse: 96239840.0000 - val_loss: 3450124.5097 - val_mse: 3450124.2500\n",
      "Epoch 196/200\n",
      "272/272 [==============================] - 0s 757us/step - loss: 151930974.6912 - mse: 151930976.0000 - val_loss: 130972314.8235 - val_mse: 130972312.0000\n",
      "Epoch 197/200\n",
      "272/272 [==============================] - 0s 826us/step - loss: 815325765.4478 - mse: 815325888.0000 - val_loss: 111391058.4706 - val_mse: 111391056.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/200\n",
      "272/272 [==============================] - 0s 748us/step - loss: 261709526.9467 - mse: 261709520.0000 - val_loss: 118994108.0294 - val_mse: 118994104.0000\n",
      "Epoch 199/200\n",
      "272/272 [==============================] - 0s 742us/step - loss: 140374941.2663 - mse: 140374944.0000 - val_loss: 25147664.8015 - val_mse: 25147666.0000\n",
      "Epoch 200/200\n",
      "272/272 [==============================] - 0s 811us/step - loss: 124983092.7206 - mse: 124983104.0000 - val_loss: 187116744.7059 - val_mse: 187116752.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a3c94aad0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습하기\n",
    "model.compile(loss = 'mse', optimizer='adam', metrics=['mse']) \n",
    "model.fit(x_train, y_train, epochs = 200, batch_size=3, validation_data=(x_val, y_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 769us/step\n",
      "mse: 256176128.0\n"
     ]
    }
   ],
   "source": [
    "#평가하기\n",
    "loss, mse = model.evaluate(x_test,y_test,batch_size=1)\n",
    "print('mse:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_13_input to have shape (4,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-dadf3e41d76e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#예측하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_prd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m57100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m58800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23995260\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_prd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_13_input to have shape (4,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "#예측하기\n",
    "x_prd = np.array([57100, 58800, 56800, 23995260])\n",
    "test = model.predict(x_prd, batch_size=4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
